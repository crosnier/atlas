

# https://www.youtube.com/watch/L-0CYzrRQX8

00:00:00.240 Welcome to the Corporate Gossip Podcast. It's the business podcast for gossip lovers. We explore the most absurd
00:00:07.120 corporate scandals, the juiciest Wall Street tea, and the not so hidden secrets of famous business people. You
00:00:14.480 don't need to know anything about business to enjoy the corporate gossip podcast, but you might learn something
00:00:20.080 along the way. Today's episode is Open AI and Sam Alman's sad sack band of
00:00:27.760 goons. speed. Welcome back to the Corporate Gossip podcast. I'm your host, CPA Scorned,
00:00:34.399 Becca Platsky. I'm here with my co-host and my brother, data analytics playboy Adam Platsky. And it's the episode, you guys. It's the
00:00:41.840 episode that we've known we've had to do for so long. And God, could it have been
00:00:47.039 any more timely? There was a huge product drop last week and everybody's talking about Chat GPT5.
00:00:54.160 Is it a flop? Is it the most incredible product that's ever seen the light of day?
00:00:59.440 You know how we feel about it already. But let's go back. Let's understand the origins of this company. This is going
00:01:06.320 to be the first in a series on Open AI because the fact is
00:01:12.080 this episode is going to be a full-length episode and we are barely going to scratch the surface. There is
00:01:18.320 too much to cover in one episode. And also, I would go insane if I tried to
00:01:24.400 cover it all at once. So, it's going to be a series of episodes. We're not going to do them back to back to back because we need to just take a breather, okay?
00:01:31.200 It's just too much. So, this will be the first of a series of episodes and we're
00:01:36.560 going to start with the main characters of Open AI. If this podcast has taught us anything, it's that CEOs are people,
00:01:42.799 too. And we have to understand their motivations, their insecurities, their traumas to put their actions in context
00:01:49.360 because oftent times, believe it or not, they're kind of not using their strategic brains to make decisions. and
00:01:57.040 sometimes they can get a little hysterical. The main character of the story is obviously Sam Alman, but we're
00:02:03.759 also going to dive into for you brainy kids out there, the NPCs. Who are the
00:02:09.598 other people that exist in the world? Who influences him? What's the drama in the executive ranks of Open AI? We're
00:02:15.599 going to get into some family drama, too. And that way when we get to the part of the
00:02:22.480 episode where he's suddenly ousted from the company as CEO and almost immediately reinstated and there's
00:02:28.160 rumors of a coup within Open AI and then you have these series of easily avoidable and extremely salacious
00:02:33.840 scandals that plague the company after he's back as CEO. You get the context. It's kind of like with Scandal. If you
00:02:40.400 didn't understand the context, you wouldn't have had any idea why it was such a big deal that somebody cheated on
00:02:46.160 a show that's full of cheaters. But you have to get the nine seasons before that to truly understand.
00:02:52.720 You know, we're just trying to pull a handy Howie Mandel over here, right? Howie Mandel
00:02:59.680 content. Yeah. But we know you guys like it. So in future episodes, we will cover
00:03:06.879 more in depth. For example, the environmental cost of AI, the human cost
00:03:11.920 of the technology, the decisions that Sam Alman is going to make and is making
00:03:17.120 to exacerbate them. Some say for his own gain. You'll get the context. And I hope
00:03:23.519 that this episode, as many of our episodes do, helps you lift the veil a
00:03:29.120 little bit because I think that OpenAI in particular is like a very Aussian company.
00:03:34.799 What do you mean by that? like like Aussie media. No, no. Like the Wizard of Oz.
00:03:41.840 Like these people are just so smart and they understand a world in a way that we don't. Peter Teal, Sam Alman, it's
00:03:49.440 possible that this is a massive scam and it's just built on hype and a wish. Um,
00:03:56.000 and I think for that, if that's the case, for that to be possible, there needs to be, you know, a boogeyman. Like
00:04:02.239 there is an Oz. there needs to be this incredible like pedestal upon which they stand and you
00:04:09.920 need to not look behind the curtain because if you look behind the curtain you realize that they're just making it
00:04:15.120 up as they go and crossing their fingers. Um, and I want you to see that the guy that's running this hugely
00:04:21.918 influential technology is just another adult loser like us. Like us, right? who
00:04:28.720 isn't always making decisions based on science or business or to benefit humanity. He has his own set of
00:04:34.160 motivations, sometimes good, mostly bad, and he uses them to make decisions and then justifies them after the fact
00:04:40.080 because he has, like Wizard of Oz, a massive communications team that
00:04:46.560 supports his every decision and spits out propaganda. So once you understand Sam Alman, I think maybe you want to
00:04:53.120 take a beat next time you hear an announcement about open AI because once you realize his patterns, you're going
00:04:59.759 to be able to see them going forward and I think it's going to give you a little bit more power as a consumer. And maybe
00:05:05.040 you just ask a few more questions. And I know by the way before we get into the episode I know I did promise you a
00:05:12.000 conversation with our start startup correspondent slash what do I crypto boy
00:05:17.280 toy Aaron Cohen because of a scheduling conflict we weren't able to get him in the studio so he will be back soon and
00:05:24.400 like I said this is going to be a series so I'm sure we're going to have him back on soon but we just could not make it happen this time. So let's get into what
00:05:32.400 Open AI is. So, OpenAI is an artificial intelligence company with a super viral
00:05:38.800 flagship product, Chat GPT. That's probably how you know them. This was released to the public in 2022. Just a
00:05:45.840 few days ago, as I mentioned, the company released the newest version, Chat GPT5. And the company's valuation,
00:05:52.560 did you see this? No. 500 billion dollar.
00:05:57.680 When the company was founded back in 2015, it was created as a nonprofit with the purpose statement to ensure
00:06:03.680 artificial general intelligence, by which we mean autonomous systems that outperform humans at most economically
00:06:10.800 valuable work, benefits all humanity.
00:06:15.840 There's some drama about whether this is a nonprofit, it's a capp profit, it's an LLC subsidiary, it's a public. It
00:06:22.720 doesn't matter in my opinion once you have CEOs who are worth billions because
00:06:28.560 of this company I don't think it's structure makes a huge difference here. Um,
00:06:35.199 I think that the concept of OpenAI being a nonprofit, maybe they started that way
00:06:40.560 and maybe I think, you know, like a lot of people that we talk about on this podcast, they started with the best intentions, but once you get enough VC
00:06:48.000 money, once you get enough influence, there's just no way you could possibly keep those uh good intentions going.
00:06:54.000 You know what the tell is for me? He runs in a circle with DVF.
00:06:59.280 100%. Yeah. when when you're running around once Barry Diller gets involved and Diane von Fenberg gets her claws in
00:07:05.919 you. Yeah. No good. As we've learned the it's the it's the uh five what is the the the
00:07:13.440 degrees of Kevin Bacon that we have the degrees of Diane von Fenberg. I don't need to see your tax of returns
00:07:19.039 to know that you are not a public benefit corporation if you are hanging out with Barry Diller and Diane Fenberg.
00:07:24.479 Absolutely. Do you want to take a stab Adam at what AI is?
00:07:30.560 From a technical perspective, AI is a machine learning model that determines
00:07:38.720 uh responses curtailed to a specific set of prompts based on uh existing data
00:07:44.800 that's out there in the universe. So um it's like a I think of it as like a
00:07:50.560 souped up version of a search engine. So, I will say I talked to Aaron about
00:07:55.919 this and he had a really great analogy or a really great description of it, which is AI. You ask it a question and
00:08:03.360 it scour the internet and it predicts based on all of the information on the internet what you would most like to
00:08:10.560 hear back, right? And I think the key here is in some cases, maybe most cases,
00:08:18.639 it doesn't actually know what it's saying. It's putting together words that it
00:08:24.000 thinks sounds right. If you go back in history, you'll see people first talking
00:08:29.840 about artificial intelligence in the mid 20th century. And you know the guy who I'm going to say his name? Arnold Schwarzenegger.
00:08:37.200 Close. Weirdly Allan Turing. Oh, Alan Turing. Yes. So, he created the Turing test,
00:08:43.279 which is if a machine can fool you into thinking it's human, then it passes the
00:08:50.160 Turing test. But I think most of our viewers might know him as Benedict Cumberbatch.
00:08:55.440 What was that? A movie? Yeah. Oh, you never watched The Imitation Game. Oh my god. But I did, you know, there's a a play.
00:09:02.080 Actually, we should go try to see it. It's called Alan Turing and the Queen of the Night and it's playing off Broadway in New York City. I think
00:09:07.760 is it like an idea of Alan Turing like going out and having sex capades doing like
00:09:12.959 But I know at the end, you know, he kills himself at the end. What was that called? The the tool that he created. It's called uh not an enigma
00:09:20.959 bomb machine touring machine.
00:09:26.399 Uh statistical probability ratio test and machine. Yeah. An electromatic help
00:09:31.760 decipher German enigma. Yeah. German Enigma machine ciphers. Yeah. Code breaking effort.
00:09:38.480 Maybe not that cool. It was cool. That is cool. I think that's cool. Um well cuz if you uh an Enigma solver
00:09:45.120 that's pretty cool. I'm an enigma wrapped in a riddle. Oh, I don't know. Is that Jane? Erica Jane.
00:09:50.640 Anyways, we're not going to cover the mechanics of AI and large language models uh today. I will throw some books
00:09:56.959 in the episode description for further reading, but OpenAI has been one of the most talked about companies this decade.
00:10:04.880 The majority of people didn't know [ __ ] about AI maybe four years ago. And now almost everyone I talk to is being told
00:10:11.839 by their bosses in whatever job they do to incorporate AI to incorporate chat GPT into every single daily task. Um,
00:10:19.519 sometimes it's helpful. Maybe you find that you're saving hours a week in doing your coding or whatever you're doing or
00:10:26.320 maybe you've been able to use chat GPT to automate tasks so you can spend time
00:10:31.680 uh working less. Or maybe you've had to maybe you've done that and then you've had to double or triple check work
00:10:38.240 because you realize that the AI has made a mistake. I went on a website recently,
00:10:43.279 a actually public speaking website because I've been thinking about getting into the speaking circuit and they had
00:10:48.640 pictures allegedly of their speakers who had six fingers. So I clicked onto that website real
00:10:54.320 quick. Um, and many people, you know, maybe the the designer who was hired for that speaker's site, uh, maybe they were
00:11:02.959 fired by that site and replaced by AI. Many people have been laid off by
00:11:08.320 companies who've made specific pro promises to investors that they were would replace X,000 jobs by AI by the
00:11:15.360 end of the year. In fact, there's a real concern that we're losing solid middle or solid or upper middle class entry
00:11:22.399 level tech jobs to AI. You just sent me a really interesting article about all the kids who were told, "Hey, become a
00:11:30.240 coder, become a prompt engineer, you'll definitely get a job." And now they're working at Chipotle.
00:11:35.440 Yeah. Yeah. Where' that girl? She graduated from Colombia. It was a good school that she graduated
00:11:40.959 to. Um, in case this is the first episode that
00:11:46.240 you're listening to of this podcast, how how are your general feelings of AI scale of one extremely doomer or 10
00:11:53.839 extremely optimistic? I'm on a spectrum between three to one and when I get to a one I
00:12:00.640 have to breathe. Yeah, I'd say I'm two. I'm not like completely going
00:12:07.279 crazy, but I have to pull myself back from the edge a lot.
00:12:13.600 Here's my take. I think AI is giving big dust bowl. Do you mind if I talk about the dust bowl for a minute?
00:12:19.440 It's your podcast. I actually promised my listeners that we'd get a whole episode about the Dust
00:12:24.720 Bowl, but I'll put some book recommendations in the show notes. Okay, just a quick Dust Bowl aside. The Dust
00:12:32.000 Bowl was, if in case you don't know, it was the biggest ecological disaster in the US of all time. Um, it was a te
00:12:40.560 decadesl long drought that killed millions of animals. It killed people.
00:12:45.760 Um, it kind of destroyed the great plains for about a decade. Um, it was also one of the reasons for one of the
00:12:52.560 biggest uh intracountry migration. um when you had a lot of people who had moved to the plains who ended up moving
00:12:58.720 to California and those were called the Okeis and the Archies. Anyway, in the run-up to the do dust bowl when
00:13:04.399 everybody was moving to the plains, there was this theory that that land was super unique and it was an ecology that
00:13:10.240 would be perfect for heavy wheat farming forever. People moved to the plains by
00:13:15.360 the thousands. A lot of the immigrants from New York and the East Coast who couldn't find jobs, they were told,
00:13:20.480 "Hey, go to the Plains, farm the [ __ ] out of it." And they were doing that for like a decade. And there was this theory
00:13:26.800 called the rains follow the plow that somehow if you plan the plowed the land
00:13:31.920 it actually encouraged rainfall. Right? That was the scientific theory of the time. Oopsie. Here you go Sandy.
00:13:40.240 There were entrepreneurs called suitcase farmers who showed up in the spring and they would plant hundreds of acres of
00:13:45.839 wheat and then they would show back up to harvest it and sell and make an easy profit. And then we had two innovations
00:13:52.480 that just blew [ __ ] up in a huge way. The first was the gasoline powered tractor and the second was the one-way
00:13:58.560 plow. Gasoline powered tractor obviously much faster than using your own body or
00:14:04.160 using a mule or using a horse. Um the second innovation was the disc the oneway t plow or the disc plow. So
00:14:10.800 normally you would use like a kind of like a fork almost that would turn the soil but it was obviously a ton of
00:14:16.079 friction. Disc plow was literally just a circle that you would cut a line in the soil and you'd go be able to go much far
00:14:21.760 faster. you could pop in the seeds. Those tools were more efficient. They covered more ground. And the problem is
00:14:29.360 the back backdraw is the soil doesn't clump up as well. It doesn't it kind of just leaves these tiny little particles
00:14:35.519 of dust um on top of the ground, which makes the ground very vulnerable during
00:14:40.959 wind season, which is all fine and dandy when you've got lots of predictable rain, which for a while there was.
00:14:48.800 And that is what caused the gray plow up. People plowed several states worth of land, but it doesn't work when
00:14:55.680 there's a drought. And there was a big ass drought for several years. And the sheer scale of the land that was plowed
00:15:04.320 up during what using these one-sided plows left 300 million tons
00:15:12.399 of dust. That's the amount of dust that was dug up to build the Panama Canal
00:15:17.839 that lifted up to the sky and floated all the way to New York and Chicago in one day. In one dust storm. That was
00:15:26.160 Black Sunday in 1935. And that dust continued to build up and continued to
00:15:31.279 cover the planes for years. For 10 years, you had the greatest ecological
00:15:36.880 disaster in American history, human created, that was only resolved
00:15:43.279 through Franklin D. Roosevelt soil conservation corps and research into soil and farming
00:15:49.279 best practices. To me, AI right now is in the great plowup period. We're
00:15:54.320 building these huge data centers. We're using millions of gallons of portable water. We're drilling into the [ __ ]
00:16:00.320 aquafers. We're unleashing chat GPT into every single workplace. People are using it instead of Google. It was the
00:16:08.720 gasoline powered tractors and the one-sided plow that enabled the scale of the farming. And that caused the dust
00:16:14.160 bowl. My problem with AI is not the product itself, it's the scale. And
00:16:21.040 sometimes I feel like a turn of the century woman in an apron and a bonnet
00:16:26.079 running around banging on pots and pans saying, "Guys, you cannot be scaling this fast." Because the second things go
00:16:32.959 bad, things are going to go really [ __ ] bad. Like there could be an ecological disaster and you could lose
00:16:38.240 power and these data centers just keep humming along, which is already happening. There was a situation in
00:16:44.720 Texas during Hurricane Harvey where a man moved his family into a data center because his house lost power and
00:16:51.440 hospitals had to move their patients, I don't know, take their patients off [ __ ] life support because they lost
00:16:57.199 power during Hurricane Harvey and the data centers still had power. And that's not a problem unless it happens on a
00:17:04.160 mass scale. We don't know what our drought might be, but the dust bowl wasn't the dust bowl
00:17:10.799 until the drought came. And the part that's even scarier to me is the guys that are running these AI companies have
00:17:16.640 the insight and the foresight of an old clump of dirt with eyes. And the
00:17:21.839 lawmakers who are supposed to regulate them are either influenced by the money that these billionaires pour into their
00:17:27.359 campaign or they're so dazzled by Sam Alman's mindbending charisma that they stop mid-sentence in the congressional
00:17:34.160 hearing and just go, "Gosh, Sam, your eyes are so blue." What was I asking you
00:17:39.440 again? Uh, here's a quote from you already know. The man we love to hate.
00:17:47.120 Josh Holly. No, sorry. The man we hate period and haunt for the rest of our lives.
00:17:52.720 Orin Hatch. Try one more time. I don't know. Melted Candle. I don't know. Melted Candle who repeats himself at
00:17:58.640 graduations. Oh, Chuck Schumer. He went on to open AI. He went to open
00:18:04.000 AI in 2019. Went to a town hall meeting and he said, "You're doing important work."
00:18:09.360 He said to employees, "We don't fully understand it, but it's important." And I know, Sam, you're in good hands. Good
00:18:16.559 soft hands. Hasn't worked a day in his life. Look at how many polo shirts that man is
00:18:22.000 wearing. Oh my god, I saw that picture. I was hoping you were going to mention that picture. If you're wearing two polo shirts, green
00:18:28.400 and red, Christmas colors, there's not going to be a callus on those hands. To be fair, that that picture was taken
00:18:33.760 in 2004. Did you ever wear double polo shirts? I was not old enough to to catch on that.
00:18:40.160 Yeah. I just want to say before you judge this picture, that was a cool look. The coolest, most fashionable girl that I knew in high school wore two polo
00:18:46.320 shirts. Five years later, those same employees that Chuck Schumer was talking to would
00:18:52.160 revolt. They would show us the depth of how much US lawmakers quote didn't fully
00:18:58.480 understand Open AI. Are you ready to dive in?
00:19:04.400 Yes. Anything else you want to say before we get started? Disassociated for a little bit. I was thinking about Farming Simulator when
00:19:10.640 you were talking about the Dust Bowl and I was wondering if I got enough friends on my farming simulator to
00:19:17.120 uh and we could create our own Dust Bowl. Does the simulator know about the Dust Bowl? I was just going to say, what was that game that got really popular during the
00:19:22.799 pandemic? Farm with friends. Farm No, Farming Simulator. People People live stream it, right? Oh, but
00:19:29.039 that wasn't during the pandemic. Um that was like FA early FA that was Yeah. I don't think I my my roommate in
00:19:35.120 college did that, but I don't ever remember her. Were you 10 to my 10 to my yield? Yeah. Doing a um farm. Yeah, I think it
00:19:41.600 was far far like that. Yeah.
00:19:47.440 Corporate Gossip Podcast is an independent podcast that runs ads from listeners just like you. Here's one. If
00:19:54.960 you live in Chicago, if you're planning to be in Chicago, if you like history,
00:20:00.080 if you like walking, if you like having fun, I think I pretty much captured everybody, then you would love Brick of
00:20:07.760 Chicago neighborhood walking tours. These tours have been consistently voted some of the best tours in Chicago, and
00:20:15.039 they only just came in second place to the Architecture Riverboat Tour, which if you know, if you've been to Chicago,
00:20:20.400 that's like the most famous tour in Chicago. Brick of Chicago tours are led and developed by architecture historian
00:20:26.480 and photographer Will Quaam and they're designed to help everybody see the beauty and design of buildings and
00:20:32.080 spaces of all types and sizes. I actually follow Brick of Chicago on Instagram and I didn't know bricks could
00:20:39.440 be so interesting. Tours run every weekend and cover 11 different neighborhoods across Chicago's north,
00:20:45.039 south, and west side. You'll learn way more about brick than you ever thought possible and have more fun than you'll
00:20:51.760 expect. But you'll also learn about history, city planning, urban design, and you'll pick up some old gossip, too.
00:20:58.799 I know you guys are going to love that. After a brick of Chicago walking tour, you'll never look at the city or the
00:21:04.799 humble brick the same way again. Come see what the city is made of. And if you want to go to a brick of Chicago walking
00:21:11.760 tour, you can go to www.brickof brickof chicago.com or click at the link in the show notes.
00:21:18.240 I know next time I go to Chicago, that is the first thing I will be doing. And that's regardless of whether Brick of
00:21:24.720 Chicago paid for an ad on this podcast or not. I was planning to do it anyway. But now hopefully I'll be able to meet
00:21:30.640 up with some corporate gossip listeners while we do it. All right, you ready for corporate
00:21:36.080 gossip number one? Yeah. Sam Alman becomes Peter Teal's hairless cat in his sweaty underground lair. So,
00:21:42.960 I really wanted to pinpoint Sam's villain origin story, and I found this 2023 New York Magazine article called
00:21:50.159 Sam Alman is the Oppenheimer of Our Age by Elizabeth Wheel. And you read this one, right? Yes. In September of 23.
00:21:57.120 So, Sam grows up the oldest of four. He's got two younger brothers, Max and Jack, and a younger sister. And they
00:22:02.960 grow up in St. Louis, Missouri. And he's like one of these boy geniuses. He gets accepted to Stanford in 2003, and he
00:22:10.400 starts this company called Looped. um which is kind of an early iteration of fine my uh kind of like foursquare and
00:22:17.679 you could share your location with friends and you know you could meet up and he ends up dropping out of Stanford
00:22:22.720 I think in his sophomore year 2005 after two years yeah so he drops out after two years to
00:22:29.360 work on it um here he is in startup school in 2008 again a American Eagle
00:22:34.960 polo and frosted tips so looped was one of the startups in the inaugural batch
00:22:40.640 of Y cominator And I don't think we've ever discussed this organization explicitly on the
00:22:47.600 podcast, but it's basically the startup accelerator in Silicon Valley. Do you
00:22:52.960 want to add anything more about Y Combinator? So, some of the companies that have been a part of Y Combinator or YC as it's
00:22:59.919 affectionately known or Airbnb, Dropbox, Stripe. I think the total companies that
00:23:05.440 have come out of Y Combinator aggregated are valued at over $65 billion dollar.
00:23:11.600 I actually would have thought it would have been more. I would have thought it would have been more too. Tons of unicorns, right? And
00:23:18.320 it's founded by a guy named Paul Graham and his wife Jessica Livingston.
00:23:23.919 I So, let's talk about Paul Graham. Paul Graham loves Sam. And again, one of the
00:23:31.280 other things that I talked to our startup core and Aaron about and he kind of clued me in on is Sam didn't found
00:23:37.600 the most successful company in Y Combinator, but for some reason Paul Graham loved Sam. Um when you'll find
00:23:46.960 out shortly that Sam ends up taking over Y Combinator and becoming Y Combinator pres president and a lot of people were
00:23:53.679 surprised because Loop wasn't that successful. Go ahead. Total value of YC
00:23:59.280 companies exceeds 600 billion. Oh yeah, I thought that was a little low. So that okay, 600 makes a little
00:24:04.640 bit more sense. So and and Sam's company Looped ends up
00:24:09.840 only selling for $42 million in 2012. So I know that sounds like a lot, guys, but
00:24:16.000 in terms of Y Combinator, it's not like super successful. In fact, you might have reason to believe that Sam could be
00:24:22.480 embarrassed by that exit. Um, but Paul Graham absolutely loves him. Here's a
00:24:28.080 quote from him about Sam. He says, "You could parachute Sam into an island full of cannibals and come back five years
00:24:34.720 and he would be the king. Sam is extremely good at becoming powerful." Paul Graham's interesting because he
00:24:41.039 talks about Silicon Valley being a meritocracy and obviously like he would be the king of that meritocracy and but
00:24:47.279 he also goes on to drop this brain buster. This is from, by the way, the book Empire of AI by Karen How, which we
00:24:54.240 read for this um episode, and I'm sure we're going to continue to reference in future episodes. So, here's Paul Graham.
00:25:01.760 Graham defended YC for not having any female founders, but said that most women had not been prepared from an
00:25:07.520 early age to succeed as a tech entrepreneur. And after analyzing the performance of applicants in Y
00:25:12.799 Combinator interviews, for which he is the deciding factor, he identified 30 or
00:25:19.200 40 factors, including, get this, a strong foreign accent, that were predictors of failure when candidates
00:25:26.000 exhibit several of them together. This was not a flaw of YC's evaluation system, but rather an important
00:25:33.679 datadriven signal, he said. He said, "You know what's crazy? I notice when a guy is wearing really
00:25:40.720 tight, khaki pants, they just tend to do better in our program. I don't know.
00:25:46.000 Somebody look into that. What does his wife think about his feelings about women? That's a great question. Here she is in
00:25:52.240 the in the picture. I mean, she's right there um with all the young men in combinator. Yeah. So, yeah. I don't
00:25:58.799 know. I mean, she's was not reached for comment, I guess, by this um by this author. So
00:26:06.000 through Y cominator, Sam gets sucked sucked into this super insular startup
00:26:12.000 fraternity of those early classes. So that includes Brian Ches, Airbnb, EMTT
00:26:17.200 Sheer, who ends up being the first CTO of Twitch, the founders of Reddit. These
00:26:22.559 are his buddies, okay? And they will defend him in these SV circles throughout his life. His other mentor,
00:26:29.679 who we've talked about at length, is Peter Teal, who's kind of collecting at this time the Young Lost Boys of Silicon
00:26:36.400 Valley. You remember around the same time what he did for Mark Zuckerberg, who bought him a car, he bought him an Infinity SUV
00:26:43.279 and he's really influencing the thinking of a lot of these guys at the time. remember like most of them are in their
00:26:50.000 early 20s. And if you've ever been in your early 20s, you remember like you're smart enough to make your own decisions,
00:26:55.279 but you're still naive enough to think that the people in your immediate vicinity are always right. You know, if
00:27:01.360 they're older than you, they must be right. Think about the way that you thought about your bosses at the time, right?
00:27:06.559 And so when he's 20, he's learning about Peter Teal's business philosophy, which was quote, "Competition is for losers.
00:27:13.840 Monopolies are good because monopolies are much more stable, longerterm businesses. You have more capital and
00:27:20.159 it's symptomatic of having created something really valuable. It's also really symptomatic of having
00:27:26.320 bribed the government. But sure, Pey, whatever you say. Oh, and here's another quote. And remember, he's telling this
00:27:33.120 to kids who probably took one history class in college. And if they're anything like Mark Zuckerberg, don't
00:27:38.240 read. He says, quote, "In the US, we had 200 years of unrivaled economic growth. We had 200 years of territorial
00:27:45.039 expansion. We had 100 years of new technology really working and people were mostly pretty happy. And now we
00:27:50.720 don't. I think that was fed into the AI that made the Prager University content.
00:27:56.159 Who was pretty happy? Pete who. This is key. Okay. He also disparages
00:28:03.679 any innovation um that quote has great benefit to
00:28:08.799 consumers but doesn't actually help the people who started these companies. As an example, he gives disc drive
00:28:14.240 manufacturing. He says, quote, "If you have a structure of the future where there's a lot of innovation and other
00:28:20.240 people will come up with new things in the thing you're working on." It's hard to read his quote sometimes. He concludes that's good for society, but
00:28:27.200 it's actually not that good for your business. That quote is funny to me because it kind of shows Peter Teal's core belief
00:28:34.240 when it comes to him and his founders, which is I deserve to make money and I'm
00:28:39.679 fine to do it at the expense of the customers. But Peter Teal and his band of boy pests
00:28:46.480 uh take it one step further because they'll also use lobbying and they'll use political influence and tens of
00:28:52.880 millions of dollars into the campaign of people like JD Vance, for example, to hamstring the government so they can
00:28:58.320 make as much money as possible. It's just [ __ ] all the way down. And then he'll say stuff like, "I'm the one
00:29:03.919 solving hard problems and the people who I'm working with are solving hard problems." Well, it's not really hard if
00:29:10.720 you're cheating, Pete. So that's our Sam Alman mindset. You know, I control the rules. I reap the benefits. But what
00:29:18.880 about the Sam Alman behavior as he's becoming more and more connected as the
00:29:24.000 investors are heaping more money at him because he's so hardworking? And I'll also add, did you watch any videos of
00:29:29.520 him? Yes, I did. If you met him on the street, what would you how would you describe him? Awkward, but not like intimidating in any way.
00:29:36.480 Would you assume he's a bad guy? No. He's just like a sweet He's a Midwestern gay man. I I
00:29:43.600 come to come into my home. He's Would you say evil Pete Buddha Judge?
00:29:49.520 Evil Pete Buddha Judge. Uh well, I mean, no, because when I if he told me about
00:29:55.279 his politics, I would say, "Oh, okay. This guy seems like he's got a good head on his shoulder." Yeah. Okay. All right. I now that I know
00:30:02.720 everything I know about Sam Alman, I do think he's evil people to judge. Um, but
00:30:07.919 so he's he's really he seems like a sweet guy. He's got these gorgeous blue eyes. He is a hard worker and we know
00:30:15.039 that investors at Y Combinator love it when you do stuff like eat so much ramen while you're working at Looped that you
00:30:20.960 gave yourself scurvy. Scurvy from ramen. You know investors would go to [ __ ] half.
00:30:26.320 That's [ __ ] crazy. They love that [ __ ] They're like, "Make yourself poor, literally kill yourself,
00:30:31.679 and that we're going to give you more money." Despite, or perhaps because scurvy can cause mental impairment, at
00:30:39.600 least according to the National Institutes of Health, he's making life a living hell for his co-workers at the
00:30:45.120 time. Here's a quote from the book Empire of AI. Twice during his time running Looped, senior leaders at the
00:30:51.600 startup approached his board and urged it to fire Alman, according to the Wall Street Journal, leveling two accusations
00:30:57.760 that would follow him all the way through his brief ouster at OpenAI. One was his tendency to operate for his own
00:31:03.679 gain rather than the companies and at times even at the expense of the company. The other was his seeming
00:31:09.360 compulsion to distort the truth. The latter was harder to pin down. He sometimes lied about details so
00:31:14.559 insignificant that it was hard to say why the dishonesty mattered at all. But over time, these tiny paper cuts, as one
00:31:21.200 person called them, led to an atmosphere of pervasive distrust and chaos at the company. Have you ever been friends with
00:31:27.120 someone who's like a um compulsive liar? No, I haven't had the pleasure.
00:31:35.840 I have. One of my friends in college was a compulsive liar and it was really disorienting.
00:31:42.480 And I remember confronting them about it and I was terrified and they admitted at the
00:31:49.039 time to being a compulsive liar and they said that they would be better and they just couldn't break the habit and I kind
00:31:55.519 of felt bad for them cuz I honestly didn't think that they wanted to do it. It was just like a
00:32:02.240 like it was compulsive. It was like they literally couldn't stop. And it it was a real bummer. Um because they
00:32:07.760 were like truly one of my best friends at the time. But it was it's kind of it's just scary. It's unsettling. It's scary. It's really weird. Paul Graham
00:32:14.559 makes Sam the president of Y Combinator. Okay. Because he's a hard worker and he has scurvy. And this is from a New
00:32:21.519 Yorker article. Did I send you this one? Sam Alman's manifest destiny. No, I did this one. So here's a quote. Y Cominator somewhat
00:32:29.360 gets to direct the course of technology. Sam says consumers decide ultimately but
00:32:34.480 enough people view YC as important that if we say we are super excited about virtual reality college students will
00:32:40.320 start cover studying it and Aaron our starter correspondent um corroborated that he said absolutely they are the
00:32:46.720 taste makers of technology especially in the valley yeah I mean if if Y cominator I picture
00:32:52.640 Y cominator as the Harvard of accelerators right and I think you said that so if Y cominator is starting and
00:32:59.760 they probably are I they are investing in any AI business they can get their hands on and not doing any CPG. Well,
00:33:07.360 then everyone's going to follow suit. And now you look, I went to a a conference not too long ago and for every
00:33:15.679 regular old uh IT company, there was five AI companies.
00:33:21.039 Yeah. Well, and here's the problem, right? because the wealth is so now extremely concentrated, you basically
00:33:27.120 have one person who's deciding everything and then we kind of have to back in because now we have all these
00:33:32.240 companies and remember they're not only being being supported by venture capitalists in Silicon Valley, they're also highly subsidized by the US
00:33:38.240 government at this point. So you kind of have this situation where it's I almost see it being like big banks where you
00:33:45.200 have these AI companies that are too big to fail. to add into that too. I mean, we know that in Silicon Valley, FOMO
00:33:51.919 runs rampant. So, if one fund is investing in in AI, then they're all
00:33:59.120 going to these guys don't have an original thought to save their lives. So, here's the thing that I think is they
00:34:06.799 don't the people that we're talking about, Y Cominator, Silicon Valley, I don't know that their main motivation is
00:34:14.159 to create an innovative business that solves problems. I think they just want to be little kings. Here's a quote from
00:34:20.399 Sam Alman after stepping down from Y Combinator to run OpenAI. The thing that I'm most proud of, he says, is that we
00:34:26.399 really built an empire. Okay, you ready to move to corporate gossip number two? Sure.
00:34:31.520 Corporate gossip number two is trust me and give me your money for your own
00:34:38.159 benefit. So, we got to move ahead a bit. Okay. I mentioned Sam Alman gets a new job running Y Combinator in 2014. He's a
00:34:45.040 big dick in the valley and by 2015 the seeds of open AI are beginning to sprout. So this story comes from a
00:34:52.399 Vanity Fair article called Sam Alman on his plan to keep AI out of the hands of the bad guys. Did I?
00:35:00.000 Yes, I sent you that one. Yeah, I have it. By Emily Jane Fox, published in 2015. So
00:35:06.000 in December of that year, Elon Musk, Sam Alman, Peter Teal, Reed Hoffman, Jessica
00:35:11.119 Livingston, I mentioned her as the Y Combinator co-founder, and other investors announced the founding of
00:35:16.560 OpenAI with the mission statement that I'll repeat here, a nonprofit research vendor aimed at developing digital
00:35:23.680 intelligence in a way that will most likely benefit humity.
00:35:28.800 I wish that they would have been like most likely. That's like preponderance of the evidence, right? So that's 51%
00:35:35.280 chance. So the founding team of openai was
00:35:40.640 handpicked by Sam Alman and I have this picture uh sad saxs clockwise from upper left we have Elon Musk, Sam Alman, Ilia
00:35:48.160 Stutz, let me get this right. Ilia Stutz and Greg Brockman.
00:35:54.880 Okay, we're going to call that him Ilia, but I I think I got it right. The time that binds all of these main characters
00:36:01.839 is that they think that AI is going to have a huge impact on society like
00:36:07.680 electricity level right and that's why at least in the beginning they make a big show of creating open AI as a
00:36:15.359 open-source organization for the public benefit of humanity which is like great
00:36:20.640 what a mission can you explain to the public then what you mean by that and here's an excerpt from the book where
00:36:28.000 author Karen How asked Greg Brockman, "What do you mean by that?" And keep in
00:36:34.960 mind, this is in 2019, four years after they founded. They had four years to figure out this answer.
00:36:42.079 And maybe should have been prepared, but I want you to imagine that you're
00:36:47.359 sitting down and here's here's the answer. Okay, this man is being paid multiple millions of dollars.
00:36:54.400 She asks, "Can you give me an example of a technology where the benefits have been successfully distributed?"
00:37:00.800 "Well, I actually think that it's interesting to look at even the internet as an example," he said, fumbling a bit
00:37:06.400 before settling on an answer. "There's problems, too, right? Anytime you have something super transformative, it's not
00:37:12.160 going to be easy to figure out how to maximize pro positive, minimize negative." Fire is another example. It's
00:37:18.320 also got some real drawbacks to it. So we have to figure out how to keep it under control and have shared standards.
00:37:24.560 Cars are a good example. He followed lots of people have cars benefit a lot of people. They have some drawbacks to
00:37:31.200 them as well. They have some externalities that are not necessarily good for the world. He finished
00:37:37.520 hesitantly his eyes lit up with a new idea. Just look at utilities. Power
00:37:43.119 companies, electric companies are very centralized entities that provide lowcost, highquality things that
00:37:50.000 meaningful meaningfully improve people's lives. But Brockman seemed unclear about
00:37:55.040 how a open III would turn itself into a utility. Perhaps through distributing universal basic income, he wondered
00:38:01.680 aloud. He wondered aloud. Perhaps through something else. He returned to
00:38:06.800 the one thing he knew for certain. Open AI was committed to redistributing artificial general intelligence benefit
00:38:14.079 and giving everyone economic freedom. Oh, I don't know. We're going to give
00:38:19.440 everybody money. Sound good? Can you get off my [ __ ] back now, [ __ ]
00:38:25.760 I'm literally falling off the ground onto the floor. Like, I am inside this
00:38:30.880 chair right now. I have taken ketamine and have melted. Like, did that make any sense to you?
00:38:38.000 No. When does it though? When when they're doing these things and they're
00:38:43.040 just talking on and on, they don't know what they're saying. They're just like, "Pop externalities there. Popping fancy
00:38:51.599 word there. Make some weird eye contact." Yeah. People will think I'm a genius.
00:38:56.800 I'm going to give everybody $1,000. Okay. So, shut the shut up. I um think
00:39:03.040 that this is what happens when technology executives exclusively go on
00:39:08.320 podcasts and don't talk to real journalists and they're never being pushed and the podcast guys are just like, "Huh?" Yeah, totally. I was
00:39:16.320 thinking about Farm Simulator. Yeah. Well, they're never being pushed
00:39:21.359 on ideas. It's just yes people. And then when they're with people who can actually push them on ideas, it's a dick
00:39:28.640 measuring contest. The way I read this article from 2015 was like, hey, you
00:39:35.680 know, they realize the implications of AI and I think this was an effort for them to be like, hey everybody, don't
00:39:42.480 worry. We got this and we know how important this is and we know the
00:39:47.520 implications of it. And government, don't think about intervening. Yeah, we can govern ourselves.
00:39:54.240 And we're pouring a billion dollars into this because we know we have responsibility and we
00:40:02.000 have accountability. We got it. We got it. We got it from here, boys. AI will solve climate change. AI will make people healthier on
00:40:09.280 healthcare. Brockman says, look at how important healthcare is in the US as a political issue these days. How do we
00:40:14.480 actually get people get treatment for people at a lower cost? That's not going to be solved by AI. Greg Brockman, it's
00:40:20.880 called Medicare for all. Greg, you [ __ ] idiot. Here's Brockman.
00:40:28.800 My friends are getting stinking rich from why don't you zip your pie hole,
00:40:35.520 dummy. So why, Greg, should we trust you guys with this powerful technology if
00:40:40.720 you can't even articulate how it might benefit us all? They say because if we
00:40:47.839 don't develop it, someone else will and they might not be as nice about it. They
00:40:54.480 might not have humanity's best interest at heart. Did you see what happened when you turn off the baby voice for me?
00:41:01.359 You're getting voices. You get veruka. Oh, wait. We came up with another idea for the Patreon. By the way, maybe just
00:41:08.079 publishing all of our episodes with Baby Voice and then the haters have to pay $100 an episode to get the Baby Voice
00:41:14.079 free episode that came out on the Patreon chat and I really liked that. And when it comes to regulators, they're
00:41:19.599 just putty in Open Eyes AI's hands. I mentioned Chuck Schumer. Here's another
00:41:24.800 interaction between Louisiana Senator John Kennedy and Sam Alman. This is after Sam Alman outlines how the US
00:41:30.800 government should regulate AI companies. You make a lot of money, do you? I make no I paid enough for health insurance I
00:41:37.440 have no equity in open AI. Really? That's interesting. You need a lawyer. I need a what?
00:41:42.880 You need a lawyer or an agent. I I'm doing this because I love it.
00:41:48.160 Thank you, Mr. Chairman. Thanks, Senator Kennedy. By the way, Sam is lying when he says he
00:41:54.319 doesn't make money from open AI. If he wanted to, he could have left that congressional hearing in his Bugatti.
00:42:00.079 So, he's not telling the truth. He didn't make a salary. But if you listen to this podcast, you know that CEOs
00:42:08.079 don't make regular salaries. That's not how they make their money. They make their money in investments and loans based on their equity, right? So, he is
00:42:15.040 making an investment through Y Combinator into Open AI and he is very much benefiting from that investment.
00:42:21.040 That's why he's got huge houses, tons of cars, and he's still making a [ __ ] ton of money. But you know what?
00:42:28.480 I don't know why John Kennedy was being so stupid about this. Look at the way he's looking at Sam Alman. So dreamy.
00:42:33.839 So get you a man that looks at you the way that John Kennedy looks at Sam Alman. We'll get into the regulatory
00:42:39.440 state uh of Open AI or AI in general towards the end of this episode. But when they get this to Congress, it's
00:42:46.640 basically the story of if you don't let us do what we need to do to grow our companies and to scale our companies,
00:42:52.640 someone else will. And their name is China. That's a good lesson. Like you can get
00:43:00.079 anything done if you just say if you don't let me do it, China is gonna do
00:43:05.839 it. And you don't want China to do it. Like if if you're a child listening to this and your mom says you can't have
00:43:11.599 ice cream, say if you don't let me have ice cream, the Chinese will. And you don't want that to happen. A lot of actually a surprising number of
00:43:18.319 children listen to this podcast. Fear-mongering is a huge part of OpenAI's public relations campaign. the
00:43:24.720 US government, they go, "Don't regulate us because if you do, China's going to get a ahead in the AI race and we will
00:43:30.640 all die." America's citizens, don't get all up in arms about how our data farms are destroying the environment because
00:43:37.119 we're protecting you from super intelligence and if we don't, you're going to die. And as far as employees,
00:43:43.040 don't worry. They had their own protocol for them. Here's a quote from the book.
00:43:48.960 The September, the year that um ChachiPT3 was released, that September, the technical leadership of OpenAI had
00:43:55.839 an off-site in a remote luxury resort. On the first night, everyone gathered around a fire pit on the rear patio of
00:44:02.160 the hotel. Let me know if this part if you if you relate to this part. Closing my eyes. Okay. Senior scientist. Now look down.
00:44:08.400 You're dressed in a bathrobe. You're flanked the fire in a semicircle. Then Ilia emerges. In the pit, he had placed
00:44:16.160 a wooden effigy that he commissioned from a local artist and began a dramatic performance. This effigy, he explained,
00:44:23.359 represented a good, aligned AI that OpenAI had built, only to discover that
00:44:28.480 it was actually lying and deceitful. Open AI's duty, he said, was to destroy
00:44:34.000 it. Ilia doused the effigy in lighter fluid and lit it on fire.
00:44:40.000 That reminds me of Tommy Boy where Chris Farley is like squeezing the
00:44:47.920 um It's so funny. My This This is the first thought that comes to mind. But you know
00:44:54.160 the kids in high school or the kids in middle school who your mom tells you not to hang out with cuz they like to burn
00:44:59.440 stuff and smoke weed. It's like that, but you're a billionaire. Yeah. These guys are such [ __ ]
00:45:05.040 burners. Like it so annoying. No offense to burners, but with the wind at their backs, their
00:45:11.839 employees successfully brainwashed, customers rooting for them to succeed, senators walking around with a half chub
00:45:18.000 just thinking about the campaign contributions they could siphon off these rich idiots in exchange for lack
00:45:23.119 regulation. Open AI was Mama D ready to rip. That's my new
00:45:31.359 That's my new vocal take. Are you ready for corporate gossip number four? Hold on. So there's uh there's one more
00:45:36.880 thing I want to cover from this article. Okay. Uh so I was when Alman was talking about
00:45:43.520 working with Elon. Yeah. It just goes to show like
00:45:49.040 he just wanted to be a part of and would say anything and do anything to be a part of that group. And remember in 2015
00:45:55.440 I think the general uh sentiment towards Elon was probably pretty healthy.
00:46:01.359 Everybody I I remember I was in co my senior year of what was supposed to be
00:46:06.560 my senior year of college. I'll put it that way. And everybody was like reading his book and being like, "Oh, Elon Musk,
00:46:13.040 the one person I'd love to have dinner with." Yeah, Elon Musk for sure. Here you go. Barack Obama has meeting with Elon Musk in the Oval Office May
00:46:19.280 21st, 2015. Anyway, so Alman says uh him and Elon get along great because they
00:46:24.560 both uh prioritize this work over sleep, which is like a huge red flag. Have a
00:46:32.960 personal life and take time for yourselves. I don't want these guys building this thing that's like
00:46:38.160 potentially going to guide society and they're completely disconnected from society cuz it's all they're doing.
00:46:44.640 We're already super distant as it is. One guy's a [ __ ] doomsday prepper. The other guy's doing ketamine shots
00:46:50.880 daily. Like, guys, be normal. I know. I know. I know. I It's sick.
00:46:57.280 This is a microcosm of the larger thing. Paul Graham is congratulating these guys. Oh, you're not sleeping. Oh,
00:47:03.200 you're getting scurvy from eating cup of noodles. Oh, you're you're urinating in a in a used Arnold Palmer container. How
00:47:10.480 much more what how much more money can I pump? Genuinely, you guys, and this is not an exaggeration because Adam and I have
00:47:16.079 seen it. these investors want you to give up everything for this company and
00:47:22.880 they think that that means that you're invested. What it actually means is they're only weeding out normal people
00:47:29.680 and they're including people who are willing to sacrifice everything. And I
00:47:34.960 think that that's on purpose. Bootstrapping as they're call they're like you got to start with nothing. You got to I want to see that you're
00:47:41.119 invested in this your personal life. What? You still have a job? Yeah. What? And think about what that does in terms
00:47:47.680 of kind of like um like a mechanism that creates the same type of founder that
00:47:54.000 I mean it's it's fraternity like it's 100% fraternity like and every year
00:47:59.040 it's the same type of dudes that are in the fraternity. Yeah. Because it's self- selecting. So now you
00:48:04.160 ready for number three? Speaking of fraternity, corporate gossip number three is horny for scale. So by 2019 to
00:48:11.280 2022, OpenAI had created the perfect environment for themselves to scale. They had done the watering and now the
00:48:17.920 weeds were sprouting. Here's a quote from Sam. Building AGI that benefits humanity is perhaps the most important
00:48:23.760 project in the world. We must put the mission ahead of individual preferences.
00:48:29.520 They got Reed Hoffman on the board at the time. And you know what that means? Master of scale. Let's scale Hoffman.
00:48:37.920 Let's scale. Let's scale. Blitz scale. Yeah. If you want more about Reed
00:48:44.480 Hoffman's blitz scaling follys, check out our Jewel episode. But effectively, it's more more. He's like a used car
00:48:51.599 salesman. That's a strategy. Integrate AI into everything. Medicine, media, middle schools, keep the cost low, keep
00:48:58.480 it accessible while it's being subsidized by venture capital money. Then once everyone is dependent, turn up
00:49:05.440 the cost. It's the Uber model. While they're growing, they're also
00:49:10.559 turning up the pressure. Right. Ilia burns a goddamn effigy in front of the
00:49:16.880 fire. And Sam Alman tells employees, quote, "We must hold ourselves responsible for a good outcome for the
00:49:23.839 world. If an authoritarian government builds AGI before us and they misuse it,
00:49:30.319 we will have failed." You better work or we're all going to die. Your family's going to die.
00:49:36.800 Everyone you love is going to die. And it's your fault because you didn't come to work jail.
00:49:42.559 Kim, there are people dying. Can you imagine? Like that's sleep
00:49:48.480 deprivation for you when you're in when you don't touch grass. when you're only hanging out with people who um are in
00:49:55.760 the Silicon Valley ecosystem and hanging out with Peter Teal who has a bunker and
00:50:01.520 most of these guys have bunkers and you're so paranoid. You actually start
00:50:07.119 to think that if you don't build this app, the world's going to end.
00:50:12.240 I'm so And it's your fault. I'm so excited in the Patreon to talk about the bunkers and stuff.
00:50:17.520 Yeah. Yeah. We're we'll Yes. So, the after show we'll talk more about the bunkers, but it's seeping into their
00:50:23.040 brain chemistry. Here's a little a little teaser for you. Ilia talks about
00:50:29.119 what he should do if his hand got cut off to be used in a palm scanner to unlock doors in Open AI. They've got a
00:50:35.440 bunker. They want to see how much it would cost to have a server room reinforced so it could withstand machine guns.
00:50:41.520 There's so much to unpack here. Imagine putting that pressure on yourself. I
00:50:46.720 mean, I go insane trying to get an episode out every Friday. Imagine if I didn't. I just said that the world would
00:50:52.160 end. I mean, that's it. Honestly, borderlines on like obsessive like
00:50:58.880 compulsive thoughts. You know what's crazy is that Vanity Fair article that you had referenced earlier from 2023,
00:51:07.040 the person who is basically stroking Sam Alman's ego on a stage, mind you, is
00:51:14.000 Jack Kornfield. And Jack Kornfield is this huge mindfulness meditation
00:51:19.920 Buddhist guru who is all about being level-headed. The reason I know this because my therapist
00:51:25.440 has been trying to get me into mindfulness and uh meditation for over 10 years
00:51:32.160 and he he he loves Jack Cornfield. So I listen to Jack Cornfield in my
00:51:37.280 headphones when I do meditation. So when I saw his name I was like what the hell? But this is not the the
00:51:43.520 actions of a person who meditates daily and touches grass nature, right? And uh
00:51:51.760 and this this is just not he's not a disciple of Jack Kornfield. Yet Jack Kornfield in this fair article
00:51:59.839 is just stroking Sam's ego. Here's one thing you'll know about Sam Olman.
00:52:05.359 He from what everybody can tell, he says exactly what you want to hear. He tells
00:52:10.640 you exactly what you want to hear. So I bet you Jack Cornfield, Jack Cornfield was told because he's a smart guy, Sam
00:52:18.720 Alman, he's definitely a smart guy. He's very well read. He knows all the right words and he's probably making it look
00:52:26.000 to Jack Kornfield that he's doing all the right things because he's very much a people pleaser. He very much wants to
00:52:32.559 be liked and people like him and and I wouldn't be surprised at all if he if he
00:52:38.079 fooled Jack Cornfeld. Yeah. So the which is kind of I think says a lot
00:52:43.520 about how good he is. If this guy is as good as So the goal for all of this if if he is
00:52:48.960 you know listening to Jack Kornfield and and he is a a mind mindfulness and
00:52:54.079 meditation expert which Sam Alman probably will say he is is that all of
00:52:59.839 his decisions should be coming from a levelheaded place where emotion is not playing in playing into it. And you can
00:53:07.839 kind of make all these decisions in your life from a completely not disconnected
00:53:14.319 state but level state. See, I think that's I don't know whatever. I think that's crazy. I don't
00:53:19.440 think it's that's possible. It but it's it's the the goal is to not have emotion
00:53:26.559 actively like cortisol like so like fear-based decisions, right? Okay. Got it. Got it. Got it. Is or like out of insecurity, right? But
00:53:35.359 I'm telling you, if he's doing that and you're trying to do what's the next right thing, right? Not just for me, but
00:53:41.920 for everyone around me. From what I can tell, do do no harm. That's Buddhism, right?
00:53:47.040 Right. From what I can tell, all of Sam Alman's decisions are based by on fear and insecurity. That's what I'm saying. So, it's in it's
00:53:53.280 just an interesting dichotomy. Yeah, it is. That's that's really interesting. So, let's move forward to when Chat GPT is released in 2022. and
00:54:00.559 they actually had no idea it was going to be as big of a success as it was. And you'll remember that that was around Thanksgiving and it was all anybody was
00:54:08.000 talking about. Sam's behavior at the time starts to become a little strange around that time
00:54:14.319 and they strike a big deal with Microsoft. I think Microsoft invested like $10 billion into OpenAI. Um
00:54:22.720 there was a team of people who felt at the time that the release was too rushed. They skirted some of the earlier
00:54:28.800 rules about safety. There was this instance where Greg Brockman's brother got access to Chachi BT3
00:54:37.280 um API to use in his video game company and this wasn't supposed to happen but the API was able to um users were
00:54:45.520 encouraging it to create uh child sex abuse material text and it was doing that right and it should not have been
00:54:51.680 doing that based on what they said it was um what it was capable of and what the rules were and some colleagues on
00:54:58.079 the leadership team described the way that Sam Alman moved around the seal as manipulative, gaslighting, and
00:55:03.599 psychological abuse. Those colleagues who said gaslighting and psychological abuse were Daniela Emodi and Daario
00:55:10.559 Emodi. They were siblings and they went on to quit open AAI and start the AI company Anthropic.
00:55:17.760 So, a lot of the resistance that was within a open AAI is starting to disappear. People are leaving. So, uh,
00:55:25.359 you you touched on safety protocols and and there I pulled something from the
00:55:30.480 New Yorker article, uh, a tech safety expert, a techn I information technology
00:55:35.920 safety expert, uh, was quoted saying basically, and I'm paraphrasing, that it's difficult to create AI safety
00:55:42.079 protocols because Altman is not being transparent with open AI capabilities. And if Alman was serious about safety,
00:55:49.200 he would be taking even the small things extremely seriously, right? And listen,
00:55:54.640 like we already know at this point that his goal is not to create the safe thing. His goal is to create the biggest
00:56:01.680 thing so that ultimately he's the one who controls it. So it's not China. So it's not anthropic. So it's not Google.
00:56:07.200 So the blitz scaling also requires them to exploit people in huge ways specifically in developing countries in
00:56:12.960 the global south. And again, this is something we'll get into in another episode, but in summary, it's kind of
00:56:18.000 like those situations at Amazon warehouses. Like people are okay with that level of human exploitation because
00:56:24.559 they don't have to see it with their own eyes, right? All you see is the package show up or the chatbot respond. You
00:56:30.480 don't see the guy pass out and have a heart attack at an Amazon warehouse and then other people have to step over his
00:56:36.559 body while they're getting your Adwala water bottle in a bag. You know, if you
00:56:42.079 saw that, you maybe wouldn't buy it. If you saw all of these people in Venezuela or Colombia or Kenya who have been
00:56:49.119 subjected to what many people call human's rights violations, maybe you wouldn't be so quick to use chatbt.
00:56:55.520 And because we don't see these things,
00:57:01.200 maybe we are able to buy what Sam Alman is saying that he actually does care about people. But as many people point
00:57:07.119 out, it's really hard to believe that Sam wants everyone to benefit. that he wants what's best for everyone when he's
00:57:12.960 being a huge pest to all the people who are closest closest with him. And the example that most people would give is
00:57:18.880 the situation with his sister Annie. And before we get into that, you want to take a break? We just we got pizza here. So yeah, we got we just got pizza. But I do
00:57:25.040 just want to talk about uh Alman's antics. I've collected You want to know
00:57:30.480 uh I don't know what I'm calling this list, but I'm just calling it Alman's antics. Okay. Uh so what does what is what does
00:57:36.480 Altman do? withholding information from the board, misrepresenting or lying to the board,
00:57:42.559 inaccurately describing safety precautions, minimizing his actions as not being a big deal, and then the board
00:57:50.400 described the conditions as an unworkable environment. We're going to get into all of those in
00:57:55.839 detail once we come back from the break, but good teaser. All right, we're going to eat some pizza and we'll be right
00:58:01.119 back. Corporate Gossip Podcast is an
00:58:06.880 independent podcast that's supported by ads from listeners just like you. Here's
00:58:12.400 one now. NeuroRenew Physical Therapy and Wellness offers expert care for every
00:58:17.440 stage, every ability, and every body in motion. The business is run by Beth Papalitzio and she offers tons of
00:58:24.720 different services like physical therapy, which includes manual therapy, task specific training, gate training,
00:58:30.640 and balance retraining. Another service that's offered is wellness services, which is a comprehensive approach to
00:58:36.319 enhancing your overall health and fitness. Whether you're supplementing ongoing physical therapy or you're
00:58:41.359 wanting to boost your general well-being, our dad, Jeff Platsky, actually was able to take a wellness
00:58:47.839 session with Beth, and I just wanted to read you his testimonial. He said, "Beth is a physical therapist operating at the
00:58:54.720 highest level. She conducts a thorough top-to-bottom evaluation that includes health history and a rigorous series of
00:59:01.119 physical and cognitive tests. But what truly sets her apart is her ability to translate those results into sharp,
00:59:07.680 actionable insights tailored to your needs. You'll leave not only with a full suite of customized exercises, but also
00:59:14.240 with a holistic road for maintaining peak physical performance as you age. Beth's thoughtful expert guidance makes
00:59:20.960 her an invaluable partner in long-term wellness. and I know he had a lot of fun doing the session, too. Beth also offers
00:59:27.359 care consulting, which is expert guidance to navigate complex medical or progressive conditions. So, no matter
00:59:33.680 where you are on your health journey, you can probably benefit from talking to Beth at Neurrow Renew Physical Therapy
00:59:39.599 and Wellness. You can find her at neurornewept.com. That's neuro
00:59:46.799 re nept.com or by clicking the link in our show
00:59:52.079 notes. All right, you back. I'm back. All right, we're back. We are fed,
01:00:00.480 energized, ready to go. So, corporate gossip number four is skeletons in the
01:00:06.880 closet and trigger warning allegations of sexual assault. So, about 10 months
01:00:12.559 after Chachi BT is released, Sam Alman becomes a household name and this article comes out in the New York
01:00:18.160 magazine. It's called Sam Alman is the Oenheimer
01:00:23.520 of our age and it's by I want to make sure I get the Elizabeth Oh, it's
01:00:28.720 Elizabeth Elizabeth Wheel. I'm sorry. I actually already talked about this article up front. So, but this article
01:00:33.760 comes out. So, Elizabeth Wheel for the first time really dug into Sam's family. Um, his mom and his brothers are doing
01:00:40.880 great. They've got nice houses. They're benefiting from Sam's success. I think at some point his brothers, his two
01:00:46.720 younger brothers actually worked with him um on his various projects. His dad actually died of a heart attack in 2018
01:00:54.000 right after Sam took over OpenAI when Elon Musk stepped down. Um, a lot of
01:00:59.200 people said he was really struggling to cope at the time and we have talked about CEOs who kind of suffer um,
01:01:05.280 traumatic losses or deaths in the family and like they kind of struggle but the one thing that they do is turn back to
01:01:11.200 work and unfortunately a lot of times they make life miserable for the people around them. Um, but the thing that was
01:01:17.280 most striking about this article was the description of his youngest sister, Annie, who had suffered from repeated
01:01:23.359 health challenges and was living in severe financial duress without housing security. Sam's and his sister's
01:01:30.640 accounts differ. There's a lot of conflict here. There's triangulation between family members. the fact that
01:01:36.799 Annie did need the money from the family because she wasn't able to have a job because of mental or physical health
01:01:43.599 challenges, but she also worried that they would like use it to control her.
01:01:49.040 Um, between 2018 and 2021, Annie's life bottomed out. That was a quote. She
01:01:54.960 faced housing insecurity, food insecurity, health insecurity. She turned to virtual and I think physical
01:02:01.359 sex work to pay the bills. Um, Annie alleges that after starting down that
01:02:07.280 path of sex work, it kind of like brought up a lot of memories that were resurfacing about Sam sexually abusing
01:02:13.760 her as a child. And there is a therapist. I don't know if it's in the article, but at least in in the book
01:02:21.520 Empire of AI, Karen How talks to Annie's therapist. I think she was seeing two
01:02:27.359 different therapists at the time to kind of like corroborate um her story. And the therapist said like it's really
01:02:32.799 common if you do start doing sex work that like things that may have happened to you in the past could start to come back up. Um if this is true
01:02:42.640 I I don't I don't know what the appropriate thing is to say assume it's true. I mean obviously you want to believe victims but at the same time if
01:02:49.520 it is true I think it's all alleged. It's alleged. But if it is true, it also means that like if Sam is a kid when
01:02:55.440 he's abusing his sister who's also a kid, I would have to imagine that would mean that Sam is very likely also a
01:03:02.240 victim of abuse because generally learned behavior you don't do that as a child unless that's happened to you. So like and even
01:03:09.119 if it didn't happen as a child to him, there was if he's doing it as a kid again, something is going on, right?
01:03:16.079 Something wrong is going on if that's if he is perpetrating that. So, it's like
01:03:21.119 this story is very difficult for me because like Sam's a kid, too. Like, this is all awful.
01:03:27.440 But one thing that's interesting about this story is contemporaneously when it's coming out, Annie's tweeting about
01:03:33.359 this. News papers are starting to pick this up and Sam goes out of his way to try to smear Annie's reputation. He says
01:03:40.720 she has borderline personality disorder. Now, the author, Karen How, reached out to Annie's therapist. Look through the
01:03:46.960 notes. She was never diagnosed with borderline personality disorder. I'm not sure where Sam got that. And Sam sends
01:03:54.160 out Open AI's chief communications officer to talk to the author about
01:04:00.880 Annie's mental health. That doesn't seem very levelheaded. Why is that in her job description? So,
01:04:08.079 the author sits down with the chief communications officer whose last name is Wong. I didn't write down her first
01:04:13.119 name. and she says, "I don't think I'm stepping out of turn here by saying that Annie has mental health challenges." And
01:04:19.760 by the way, the author notes that at this point she had not brought up Annie at all.
01:04:25.440 Oh, like she's just like unprompted. Unprompted. She goes on, "Annie has some good days
01:04:31.359 and some really bad days, and the family is trying very hard to strike a balance between protecting her and not enabling
01:04:36.880 her." Here she reiterated the point for emphasis. Notice the family hasn't put out any public statement denying what
01:04:43.119 Annie has said. It all comes back to protecting Annie. Like this is an AB
01:04:48.799 conversation, lady. CC o your way out of it.
01:04:54.400 CCo your way out of it. That's good, Adam. And also like why would you think that any self-respecting journalist
01:05:01.039 would take what the chief communications officer has to say? Like have you even met these people? So this is from Empire
01:05:08.640 of AI. Annie's story deepens the dueling portraits that people paint of Sam. He
01:05:13.680 is at once generous and self-serving, agreeable and threatening, a benefactor for so many people and the source of
01:05:19.359 great personal pain for others. Someone who projects sincerity and altruism in public, but reveals a more complicated
01:05:25.680 calculus through his behaviors behind closed doors. someone who can give and take away, leaving many with an
01:05:31.200 impression that they are part of a larger game of chess for which only he can see the full board and the endgame
01:05:36.960 is to preserve his power as king. Thought that was really profound. Mhm.
01:05:42.079 One thing that Aaron brought up again in our conversation that I hadn't noticed was in a letter or a statement um that
01:05:51.280 the Altman family had published about Annie, they signed it, Max, Jack, Sam,
01:05:57.200 and Mom, right? So, the rest of the family all signed basically saying these
01:06:02.720 allegations are untrue. Annie is mentally ill. You know, don't don't listen to what she has to say. And he
01:06:09.119 brought up a good point that like we don't know whether the family is just trying to protect their own wealth at
01:06:16.079 this point. Like has Sam told them, "If you guys don't agree with me, I'm yanking the bank on you guys, too."
01:06:21.920 Mhm. I don't know. He certainly has leverage over them. Yeah. I It It's definitely a thought.
01:06:27.280 I mean, it's kind of crazy. And it's like you you got to wonder, especially the way that he Listen, I've said it before, like the way that you act in
01:06:33.520 real life is the way that you're going to act at work. like you cannot sever your personality and become a completely
01:06:38.960 different person. So that's why like the things that happen outside of you carry inside and vice versa. Um
01:06:45.839 I have a hard time believing that if he's gaslighting manipulative to his the people that he works with that he
01:06:51.599 wouldn't also be that way with his family of origin. Another kind of sad part of this story is Annie said, I
01:06:56.799 don't know if it came up in this article, but she had said like her dad was like the one person that got her and the one person who was her supporter.
01:07:03.280 And when he died, it kind of like, you know, Sam has also talked about that time being like the worst time in his
01:07:08.720 life, but like for her, she's like, I lost my protector. The rest of the family's against me.
01:07:16.160 Um, Annie ultimately filed a lawsuit against Sam in Missouri earlier this year, 2025. Um, she just turned 31 and I
01:07:24.400 guess the statute of limitations is 10 years after you turn 31. Sorry, 10 years after you turn 21 or when you're 31, the
01:07:31.599 statute of limitations expires for a sexual assault case from when you were a kid. So, she filed it basically right at
01:07:38.799 the deadline. She suing for damages of 75,000, which I
01:07:44.720 think is the minimum that you can ask for in these cases, which makes you think like she's not really here for the
01:07:51.200 money. She just maybe wants to be heard or at least that's what that action at least that's what she wants to like convey with doing it that way.
01:07:57.440 Seriously. Yeah. So, what do you think about all this? Um,
01:08:02.640 she also wants a jury trial, which I think is quite interesting because it's like she wants the evidence out there. She wants a jury of her peers to decide
01:08:09.280 whether he's liable for this or not. Yeah, it's tough. It's really, really tough. I mean, you know, certainly f all
01:08:15.920 family dynamics are different. I think it's difficult to see somebody
01:08:22.319 you love and you grew up with go through something hard and like to me it's like yeah I
01:08:29.520 want to throw I'll just throw money at it. Like I I have the means to give you the best mental health
01:08:35.759 that money can buy, right? Uh but sister doesn't really want that. She does just want she she kind of it
01:08:42.158 sounds like it just needs to be like what she's looking for is some kind of account back to accountability to like
01:08:48.560 work through. Let me pause that. She does want that. So she there was a time when Sam was paying her rent. There was also a trust
01:08:55.520 that she was given from her dad. But I I do think there is some like hey Max and
01:09:00.640 Jack have great financial stability. Mom has great financial stability. Where is mine? And like clearly she's struggling
01:09:07.279 so much with housing insecurity that she's turning to sex work. I think it seems like what she wants is no strings
01:09:12.319 attached. Yeah. No strings attached. And I think her family is wanting from her perspective again, her family is wanting
01:09:18.080 to control her, use this therapy program. Go to this go to this institution D. And she doesn't
01:09:25.040 want that. She's like, I just need to be supported. I want to I know she's it seems like she's an artist. She's like, I want to do my art and I just want to
01:09:31.759 live my life away from you guys, but I cannot get a job like a normal person.
01:09:37.359 Yeah. and the family the f you know the family is trying to control them. So they they you know what
01:09:43.759 they should do and this is what's recommended in these situations is just like let the person do what they need to
01:09:50.560 do like you kind of just have to give up your control and let the person individually who they recommend it
01:09:57.040 I would say mental health providers or mental health it's like it's like Alanon stuff
01:10:02.080 right it's like you the mental health stuff like the mental health stuff comes first
01:10:07.440 and then there's like you know drugs and alcohol are used later to suppress the the mental stuff. So,
01:10:14.159 it's difficult. I I my heart hurts for her. I There was a quote that she said
01:10:20.880 she was suicidal at six, which there's something there. There's
01:10:27.600 something there. And obviously, there's what was alleged, but no kid should come
01:10:34.400 out of, you know, the womb and be in their family environment. and you're a child, you're six, and to be suicidal,
01:10:40.960 to even have those ideations is is is is really really sad.
01:10:46.000 All right, let's move on to a different topic, shall we? Corporate gossip number five is who knows, Sam might be a shitty
01:10:54.000 person, but we know for sure he's a shitty manager. So, I want to introduce
01:10:59.199 one final person in the story, another woman. Her name is Mera Morati and she was an executive on the OpenAI team and
01:11:06.400 she worked with Ilia and Greg and Sam. According to the book, she is the only one on the team who has influence on
01:11:12.640 Sam. She is the only one who can tell him straight up that his plans were unrealistic and he would actually
01:11:17.760 listen. She was honest, says a former colleague. She was the one getting things done.
01:11:24.400 The problem with the being the only person in this band of goons who can talk to the head goon is you're
01:11:29.920 constantly having to clean up the mess. Sam is very conflict averse. He reminds
01:11:35.120 me a lot of Tony Shay. If two teams disagree, and by the way, that's um I'm referencing Zapo's founder
01:11:42.719 who I'd actually want to do that episode next. Um if two teams disagree, he goes
01:11:48.400 to each of them and says, "I agree with you. The other team is wrong." And Greg Brockman apparently ends up
01:11:55.040 encouraging this behavior. And there are situations in which OpenAI is giving a presentation that Meera puts together to
01:12:01.760 Microsoft, their biggest and most important partner about the commitments that they can make and Sam just goes
01:12:07.199 completely off the script in front of her and starts making commitments that aren't on the road map um that that
01:12:14.159 isn't even possible about what they can do. And the worst part is um when the board is like, "Hey, Mera's a little
01:12:19.679 upset about how the meeting went." Sam says she just doesn't have as good of a relationship with Microsoft as I do.
01:12:27.360 You know what's so frustrating is it's like they will say that like Meera is
01:12:32.400 getting stuff done. She's managing. She's productive and she's dealing with the [ __ ] of these [ __ ] dweebs.
01:12:38.880 And then they'll say, "Why are you so stressed out? Like, aren't you doing the same amount of work as both most of us?"
01:12:43.920 It's like she's doing an amount of emotional labor that you wouldn't even be able to fathom and still doing her
01:12:49.040 job. It's unbelievable. Alman only becomes more insane as he
01:12:54.080 gets more famous. Uh he's, you know, he's got an overwhelming travel schedule. He used to be super energetic
01:12:59.280 and now he's always exhausted. Yo, what about the world tour? The Altman world tour.
01:13:04.480 It's going to be bad for the tour. He's pulling a uh Justin, what would be the corporate equivalent of of Justin
01:13:10.719 Timberlake? No, I don't want to. Oh, I'm looking right at the
01:13:16.159 I thought you were talking about the world tour example because he's pulling a Mark Zuckerberg with his altman world
01:13:21.199 tour. Yes. Yes. Yes. Exactly. All these guys got to run around. They got to have their policy teams. They got to meet
01:13:26.480 with CEOs and executives and do all the hobnobbing. And his anxiety is getting
01:13:31.760 worse and it's fueling his patterns of destructive behavior. He's agreeing with people to hit their face. And now with
01:13:38.719 quote increasing frequency, he's bound bathing them behind their backs. He's creating conflicts. It's contagious. His
01:13:45.679 This is worse. His direct reports are seeing, oh, this is how businesses are done. And they pitch their direct
01:13:51.040 reports against one another. What is a Bravo comparison to this? Who
01:13:56.719 does this? It reminds me of Mike. We just actually watched Mean Girls cuz Mike had never seen it. I know, but it's like the
01:14:02.880 scenes where it turns into the jungle, right? It's exactly like that. that was coinciding with mounting competition
01:14:11.040 externally. You know, Anthropic is on their tails. Google is on their tails. We know from Mark Zuckerberg episode how
01:14:16.960 insane these founders can get when things aren't on the up and up. That's why you need a monopoly reinforced by
01:14:23.920 laws that you develop to protect you because you Sam Halman cannot handle the pressure. Sam starts pushing people, go
01:14:31.520 faster, ignore safety protocols, ignore processes, keep pushing new models, keep
01:14:37.120 the press positive because all this press is getting down to me. Everybody, we're in work jail now. Weekends,
01:14:42.719 nights, you're not allowed to leave. No overtime. You do what I want you to do. Kids can visit you on the weekends. He
01:14:48.640 tells Meera, this is key. I talk to legal. Chat GPT4 Turbo is good to go
01:14:56.320 without review. But when Meera talks to legal, they go, "What the [ __ ] We never
01:15:01.440 said that. We never said it wasn't didn't need a review." And she finally sits him down and she says, and I'm sure
01:15:08.320 very tactfully, she says, "This is what you're doing. This is how it's affecting the team. This is how it affecting me.
01:15:14.239 It's got to stop." And what does he do in retaliation? I'll give you a clue. Same thing that Tony Shay did.
01:15:20.000 Fired her. Ice her out. Yeah, he ices her out. Here's a quote. Meera had seen him do something similar with
01:15:25.840 other executives. If they disagreed with or challenged him, he could quickly cut them out of key decision-making
01:15:31.120 processes or begin to undermine their credibility. Inevitably, different executives had each had their turn
01:15:37.280 bearing the brunt of this treatment. Over time, the cumulative impact of his actions had taken its toll on the
01:15:42.880 highest levels of the organization. Meera has enough. And she brings the situation to the board. She says Alman
01:15:50.159 is a very anxious person and when he feels anxious he has dumb ideas particularly when enabled by Brockman
01:15:56.800 disgusting brothers. Alman's anxiety Alman's anxiety is also fed into toxic
01:16:03.679 behaviors that always follow the same playbook. To anyone resisting his decisions, he says whatever he thought
01:16:10.239 things thinks they want to hear and they win. He wins their support. And then when he loses patience waiting and
01:16:17.360 believes that they're going to go against him, he will undermine their credibility until he they got out of the way.
01:16:22.640 Speaking of the disgusting brothers, did you see that one of somebody an unnamed
01:16:28.400 source compared Sam Alman to Tom Wamsgam as like his demeanor?
01:16:34.640 So interesting. Yes. His demeanor. His demeanor. That's what I'm saying. Like he's a guy you're like, "Oh, this guy's
01:16:39.679 fine." It was so funny. I was like, "Okay, okay. I see. That's what I'm saying. He's evil people judge. Is he not? It's
01:16:46.080 the same guy. Um I'm sure. No. It seems like your [ __ ]
01:16:53.040 corporate life is very charmed. Have you ever worked in a toxic environment? Oh, me?
01:16:58.080 Yeah. Uh toxic. Oh, yeah. I did work. Oh, yeah. Yeah. Yeah. Super toxic environment.
01:17:04.239 It's exhausting. Like I will say the time I cried. The times that I've been in the lowest in terms of my mental health was
01:17:10.480 when I had, and I hate to say it, but I've said this actually just on Norah's podcast, uh, Nora McCannernney I was on.
01:17:16.239 Thanks for asking, a couple weeks ago, and I said some of the most toxic environments I've been in, women bosses
01:17:21.840 who I believed in and who I thought I had a good relationship with, and then ultimately they turned toxic and it felt
01:17:27.520 like such a betrayal. It was awful. It was awful, awful, awful. Oh, I still dream about seeing this one boss in
01:17:34.239 particular who's incredibly toxic and her little sidekick. And I dream about like what I would say to them because
01:17:40.560 they were so mean to people. They were mean bullies. Awful. And you have night tears about that.
01:17:46.239 I dream I have dreams about them, but not and not where you're speaking and saying, "No, I have I guess I have daydreams
01:17:52.480 about it, but it's just like they were just so mean to so many people who I thought were great." I have the same thing, but mine turned
01:17:58.000 to be night tears. It's interesting. like where I'm screaming at them. Um, apparently your night tears
01:18:04.000 actually, um, a lot of people related. They also have night terrors. Yeah. Yeah. Yeah. So, um,
01:18:10.560 but a lot of people said they didn't want to pay to hear them. Well, it's unresolved trauma. As I was
01:18:15.679 saying, I'm a work in progress. Jack Hornfield hasn't solved me yet. It's true. Um,
01:18:21.760 it's really hard to get work done when you're in a toxic environment. And especially if they are doing the most
01:18:27.440 lord's work, the most important work on the face of the planet where everyone else dies. And imagine you're in a toxic
01:18:33.280 environment. I would say when I was in a toxic environment, I spent 80% of my time trying to manage my co-workers and
01:18:40.719 my boss and being anxious about what my boss thought and being anxious about what she was going to say and 20% doing
01:18:47.199 actual work. you you cannot get work done. You just feel awful. And Ilia snaps.
01:18:55.120 And remember, he has fully uploaded his brain to the Matrix at this point. He's worried about the way that AGI will
01:19:00.480 impact humanity. And now he's wondering, I told everybody that if we don't reach
01:19:06.400 AGI, then we all die. And now I don't even know if we can do
01:19:11.760 it. And if we do do it, should Sam be the one that gets us there? And then he
01:19:17.280 had the family matters moment. What was that? Did I do that? Did I do that?
01:19:24.320 Yeah. [ __ ] I enabled a psychopath. He goes one, two, wait. Three, two, one,
01:19:31.360 one, two, three. What the heck is bothering me? Sam is being cuckoo crazy.
01:19:38.480 Meera and Ilia agree. I don't think Sam is the guy should have the finger on the
01:19:43.760 button for AGI. Ready for corporate gossip number six? Yes. Who said this? I don't remember.
01:19:51.040 I'll I'll insert it maybe in post. Fame is a mask that eats the face by John
01:19:56.080 Updike. With chatbt's monstrous success, Sam is peing. He is Times 2023 CEO of
01:20:03.360 the year. Working at a open AI, according to the book, is the ultimate social currency in
01:20:10.560 Silicon Valley. Like they're probably going to which strip clubs. You know what? I did not go to a ton of strip clubs in San Francisco. Really? I
01:20:17.280 wasn't running with that crew as much, but I was hitting But you know them, don't you? No, I was I was really a part of the bar
01:20:23.600 scene there. Um, if anyone is from the the Missouri Lounge when I was living in
01:20:29.679 Berkeley. Yeah. Love the Missouri Lounge. And then San Francisco Athletic Club when I lived in
01:20:35.040 lower pack Heights. So, this has the Gold Club. Have you heard of that one? No. Like I said, I didn't I didn't hit a
01:20:40.159 single strip club in San Francisco. I was so close to Vegas. Why get the sample spoon when you can have the
01:20:46.000 kitchen sink? Right. Right. No, that makes sense. So maybe they were I was I was an hour door to door to
01:20:52.239 Vegas for me. If you were in San Francisco in 2020 at the any of the strip clubs, I guess you
01:20:58.880 know what I had to wear my mask. I was going to say give me your thong. I'll put it on my
01:21:04.239 mask. Safety first. Ilia describes Open AI at the time,
01:21:11.760 inside the kingdom, things are falling apart. And he describes Open AI as a directionless, chaotic, and backstabbing
01:21:18.719 environment where people no longer had shared information or a shared foundation of trust to agree on critical
01:21:24.640 decisions on how to move forward. This infighting was slowing down research progress and eroding any chance at
01:21:30.800 making sound AI safety decisions. Mhm. One thing you have to know about Sam is
01:21:35.920 he does not want board of directors oversight. He thinks that the board of
01:21:41.440 directors should be um just give him advice and but give him a wide birth. Okay, that's the thing that he was
01:21:46.880 telling Stanford students u back in like I think 2017 he had given a talk. That's
01:21:52.400 his mantra. Okay, that's his Peter Teal hat on. He doesn't want any oversight
01:21:57.840 from the board of directors. He doesn't want any criticism. Excuse me. Sorry. pizza and pizza.
01:22:04.159 He doesn't want any oversight. He doesn't want any criticism. So when one of the board members, Helen Toner, she's
01:22:10.080 a scientist, she publishes a research paper that has one singular criticism of
01:22:15.360 Open AI, it's like on page 35 of this 65page research paper and
01:22:22.880 Sam rips her a new one and it's ultimately leaked to the press. People
01:22:28.400 think that Sam leaked it to the press because there's no way that an already strapped newsroom is going to go find a
01:22:34.239 [ __ ] 30page, 60page research paper and be like, "Oh my god, the the board
01:22:40.320 of directors is criticizing OpenAI." Because that's a normal thing for board of directors to do, by the way. So he
01:22:46.000 pulls the same [ __ ] again. He calls her up. They have a good conversation. They both seem to agree that maybe he should
01:22:52.159 she should have given him a heads up, but overall a board member should be able to criticize the company. But to
01:22:57.840 everybody else, he says, quote, "Any criticism from any board member is
01:23:03.120 unacceptable, and she's got to go." And then he starts to triangulate. He tells
01:23:08.320 each board member and each executive that each other thinks that Helen should
01:23:13.760 leave. Wow. That's that's vindictive [ __ ] man. That's some survivor [ __ ] when you
01:23:19.360 Yeah, that is survivor [ __ ] That is totally survivor. He's causing quote
01:23:24.639 fragmentation among his own leadership team scattering information to different people but never giving anyone the full
01:23:30.159 picture allowing him to maintain full control. Is that Siri? Would she do that?
01:23:35.199 Siri wouldn't do that but Parvity would. Parvy. Yeah. Yeah.
01:23:40.719 So, this only works if the board members
01:23:46.560 hadn't caught on to his behavior and he thinks he's going to outsmart them. The truth is they don't normally talk to
01:23:51.760 each other. But now because they had that insight from Ilia and Meera and because they're kind of putting things
01:23:57.520 together, you know, with a toxic box, sometimes you think it's only you. Sometimes you're like, I'm must be the only person seeing this. But then you
01:24:04.000 start to talk to other people and you realize that it's a pattern and you start to put together the pieces and they had done that. And so now they have
01:24:09.440 back channel communication where they could compare notes and get the truth. And this was the last straw. The board
01:24:16.320 was like, if we cannot trust this guy, we have to replace him. Here's from the book. The independent directors laid out
01:24:22.880 what they knew. In total, the three of them had had similar feedback from at least seven people within one or two
01:24:29.280 levels of Sam, including Ilia, Meera, and the Emoi siblings who oversaw safety
01:24:35.600 and non-safety parts of the company. Several had described Sam's behavior as abuse and manipulation. Most had
01:24:42.480 highlighted his lack of honesty and their inability to trust what he had said. There were myriad other issues
01:24:48.719 that the independent directors had found, including the disempowerment of the nonprofit. Sam not disclosing his
01:24:54.400 legal ownership of the OpenAI startup fund. That's how he makes money. And Sam lying about legal approval for GPT4.
01:25:01.920 What did it mean that OpenAI was trying to build AGI when its senior leadership
01:25:07.120 couldn't trust either either basic or critical information coming from its CEO? Plus, Mirror is doing all the work.
01:25:14.639 Everybody trusts Meera. Most people don't trust Sam, so we don't really need Sam anyway. So, they finally catch him
01:25:20.880 in a lie. Straight up. This is like right out of succession. Sam told Ilia that one of the board members, Natasha
01:25:27.360 Macaulay, who fun fact is married to Joseph Gordon Levit, was supportive of removing Helen Toner from the board. But
01:25:34.480 now they're all in a room together. And Natasha says, "I never [ __ ] told you that." Here's a quote. On the second day
01:25:41.120 of the five-day board crisis, the directors confronted Sam during a mediated discussion about the many
01:25:46.560 instances that he had lied to them, which had led to their collapse of trust. Among the examples, they raised
01:25:52.719 how he had lied to about Natasha, saying Helen should step off the board. Alman, realizing he's been had lo momentarily
01:26:00.480 lost his composure, clearly caught red-handed. "Well, I thought you could have said that. I don't know," he
01:26:05.760 mumbled. "You done [ __ ] up, Sammy boy. November 17th, this is a Friday, the
01:26:11.840 board of directors made their decision, remove Sam, put in Meera. They push out a statement that said Sam Alman had, do
01:26:18.800 you remember this quote? Not been consistently candid with the board.
01:26:24.000 So, here's the problem, though, and this is how I know I I think these guys have not read a history book. You cannot just
01:26:29.120 take out the leader of what I'm going to be calling a cult when all of the cult members are still
01:26:36.239 brainwashed. And you can't just tell them he's been lying to us because this man is a master
01:26:42.719 manipulator. What did Paul Graham say at the beginning? He would drop him into an island of cannibals and he would come back their leader. The problem is all of
01:26:50.800 the execut all of the employees other than those top levels of exe executives were still bought in.
01:26:57.760 So the other thing is he had just been named CEO of the year.
01:27:05.199 No regular customers knew that he was being manipulative. All of the um
01:27:10.480 senators at the time, I mean, look, they were they had John Kennedy practically had like hearts for eyes. I mean, they loved this guy. I
01:27:17.760 remember when I found out that he was fired, I thought, that's weird. This is a super popular product. Like, why would
01:27:23.920 they fire not being consistently candid? What does that mean? Like, you're going to have to tell me so much more.
01:27:31.520 The problem is when all the executives or when all the employees ask those
01:27:36.880 executives and those board members, can you give us any evidence? They can't
01:27:42.000 because if they did, they'd be throwing Meera under the bus. They'd be throwing Ilia under the bus and they can't do
01:27:47.440 that, right? So, they're kind of in like a tough spot. And if you were somebody
01:27:52.960 who had been manipulated by Sam Holman, you're not going to stand up in that moment and say anything because you know
01:27:59.440 if you go against him and it maybe looking at this point maybe he'll come back. You don't want to be on his bad
01:28:05.120 side. So you're not going to stand up. And so at this point Ilia and Meera start to freak out. Did we make the
01:28:10.400 wrong decision? I don't think curious what you think. I don't think they made the wrong decision. I think they're just bad
01:28:16.320 leaders. I think they're just they they didn't think about the consequences.
01:28:23.760 Yeah. I think you got you got to lay it out there. You got to you got you got to can't be afraid of making looking bad.
01:28:31.199 Listen, look look like my man Omar says, you come for the king, you best not miss.
01:28:37.600 Aaron said the same thing when I talked to him about this. Yeah, it's absolutely right. Um,
01:28:44.960 so I guess if you were in the mafia, what would be the way to go about this?
01:28:50.159 Oh, I don't know. Because this is very mafiaesque, right? Yeah. I mean, it's it's a coup, right?
01:28:56.880 Right. But you need some you need you just got to air it out. You just got to you got but you got you got to put it
01:29:03.360 to me. It's like you got to communicate it down so that the soldiers are are aware and are aligned, right?
01:29:09.120 And then when you give the announcement, they all go, "Yeah, this is a good idea. Trust me. Yeah. Yeah, you're absolutely right. Greg
01:29:15.280 Brockman quits in protest. Employees sign letters saying they're going to quit if Sam isn't brought back. And remember, these guys have money on the
01:29:22.320 line, right? Um, all these companies, Microsoft, Google are coming to OpenAI
01:29:28.239 trying to poach their best employees. Um, without those employees, the company
01:29:33.520 is going to be a shell of itself. And all of that equity that all these um
01:29:39.520 employees have in the company is worthless. So Meera finally relents. She says, "Fine, I won't send him back. This
01:29:45.679 is not this is the whole company's going to collapse." Now, here's a description about what happens next from the book.
01:29:52.159 Employees put together an open letter protesting the boy's decision, threatening to quit. Uh Meera signs the
01:29:57.840 letter. Under an onslaught of questions, the Ford board members repeatedly evaded making further disclosures, citing their
01:30:04.239 legal responsibilities to protect confidentiality. Several leaders grew visibly enraged. You're saying that Sam
01:30:10.480 is untrustworthy? A vice president of global affairs said that is just not our
01:30:15.920 experience with him at all. And at this point, this is important. All the other employees who left the
01:30:22.560 company in frustration were quiet. Nobody comes out of the woodwork and says anything to corroborate the board's
01:30:29.600 allegation. All of the former employees who might have quit because they weren't unhappy with Sam were silent. Remember
01:30:35.840 that fact. Jason Quan, the chief strategy officer, um, upped the ante. It
01:30:41.840 was, in fact, illegal for the board not to resign because if the company fell apart, it would be a breach of the board
01:30:47.840 members fiduciary duties. So, a couple days later, the board announced that Altman would be back. The three
01:30:53.120 independent board members who were in the meeting when they confronted Sam about his lives would go. He Sam Alman
01:30:59.120 did concede to an independent investigation and Sam was back. Um by March 8th, the
01:31:06.880 new board members concluded the investigation. The report was never released to the public public. Larry
01:31:12.400 Summers, who is now on the board, said that they did find lots of instances of Sam telling different people different
01:31:18.320 things, but it wasn't enough for him not to run the company, which is classic. And why would I [ __ ] trust Larry
01:31:24.000 Summers who mentored the biggest manipulative gaslighter of all time, Cheryl Samberg? Um, and also like it
01:31:31.120 like of course if you weren't there and you're looking at the individual instances, you're not going to think it's that bad. It's all in context.
01:31:37.199 That's how abuse works. And this is the cursed selfie of Sam
01:31:43.199 Alman returning to Open AI like a [ __ ] idiot. Um,
01:31:49.920 so now Sam is back at the top. The company is filled with loyalists and Sam goes on a victory tour. Open AAI is
01:31:56.159 still struggling internally, but the more challenges they faced, the more Sam would quote overcompensate with public
01:32:03.280 declarations of its extraordinary success. Love bombs.
01:32:08.960 Corporate love bombs. And keep that in mind when he's telling you that Chachi BT5 has PhD level intelligence and it
01:32:14.960 cannot solve a simple math problem. It was becoming a pattern. If Sam was being brazen and boastful, most likely it
01:32:22.560 meant something wasn't going well and things were not going well. There was the Scarlett Johansson lawsuit where
01:32:28.239 Open AI basically uses a copy of her voice after she declined the offer to work with the company to build a voice
01:32:34.719 chat. They asked me to. They wanted to just do full baby voice. Full baby voice. Um the New York cop New York Times
01:32:40.560 copyright lawsuit news was pouring out about the open AI's environmental and humanitarian exploit uh exploitation.
01:32:47.600 That was when the article came out about the thousands of Kenyan workers who had been basically working in slaveike
01:32:54.000 conditions to train the data for this model. Um, and at this point inside the cop cult of open AI, when everyone else
01:33:01.040 is against you, you stick together even more. It's us against the world. But then finally something happens that
01:33:07.840 causes even Sam's biggest believers to crack. Oh, one thing to note is a reason
01:33:14.239 that a lot of these employees working these insane hours and crazy conditions going to work jail is that they've got
01:33:20.560 all this equity and they're promised a massive payout when Open AI either goes public or is bought. So, they're like,
01:33:26.560 "You know what, Sam? You might be insane, but you're good to me, so I'm going to stick it out for the payday."
01:33:31.840 But then this Fox article comes out in spring of 2024. It's called leak open AI
01:33:37.520 documents reveal aggressive tactics towards former employees. In it, Kelsey Piper explains employees at tech giant
01:33:45.120 OpenAI who wanted to leave the company were confronted with expansive and highly restrictive exit documents. If
01:33:50.960 they refused to sign, they were reportedly threatened with the loss of their vested equity in the company,
01:33:56.080 which could be, by the way, like millions. This is very uncommon in Silicon Valley.
01:34:02.000 The policy had the effect of forcing ex- employees to choose between giving up what could be millions or agreeing not
01:34:08.480 to criticize the company with no end date. So that's why no employees came out to corroborate the board stories
01:34:14.080 about Sam. Anyone who left the company was gagged.
01:34:20.000 Employees who were still at the company had no idea and they pressed Sam. Did you know about this? Yes or no? In a
01:34:25.679 town hall. He goes, "I knew about it in some instances, but didn't realize it was a requirement for everyone. It
01:34:32.159 escaped my notice." [ __ ] Brockman chimes in. Same. That's exactly what I wrote. Bull [ __ ] [ __ ] More
01:34:38.639 evidence came out that the executives knew about it, signed off on the claw clause. Um,
01:34:45.920 employees continue to push. What the hell? This article came out. They say you knew. He says, quote, "We're still
01:34:51.600 trying to get a full understanding of the scope, but you know, it's our names on the document. We were in conversations where these tactics were
01:34:57.679 discussed. All right. I hate to say it. He admitted it. Well, but he's also doing remember what Ted Cruz says, when you use the passive
01:35:04.719 voice, it's because you're trying to avoid responsibility. Yes. But admission is step one.
01:35:11.280 Okay. They said they had emailed all of the former employees, releasing them from NDAs, and they had taken off the
01:35:17.760 provision for newly deployed employees. And then, of course, another investigation. Now, here's this is kind
01:35:24.880 of a funny quote. Jason Quan, he's the guy who said, "If you don't bring back Sam back, it's illegal. You're going to
01:35:32.560 real business jail." Um, this is from the book. Jason Quan, when
01:35:38.000 all these employees are freaking out, they're raging. Sam, tell us the truth. It's like, you know, Beauty and the
01:35:43.040 Beast where they're knocking on the door. Jason Quan gets up and goes, this is his stilted, heartfelt defense of
01:35:49.040 Altman's characters that seem strangely out of place. And imagine, I want you to imagine you're as angry as you could
01:35:54.159 possibly be. You feel betrayed. You feel confused. I thought I was working for something good and now I realize you might be a liar. And some guy executive
01:36:02.320 comes up and says, "Guys, guys, guys, we want to give you answers when you ask for them because we know you want them
01:36:08.639 like right now." And I think sometimes we try to give them to you and you know, we should just wait sometimes. And I
01:36:14.639 think that like that is like that is part of part of the whole thing that's happened here. It's not that there's
01:36:20.480 intentionality sometimes in all of this. It's just I I I I really think, you know, Sam in particular, he just doesn't
01:36:26.639 want to let you down. That's where it comes from. Like I've been working with this guy for a really really long time. That's why I keep working with him, you
01:36:32.639 know, and so it's just it just come it does come from a good place. That's what
01:36:38.080 I'm saying. You can [ __ ] talk me all you want, but you know, that's uh uh uh yeah, that's what I got to say.
01:36:43.280 Nice. It's a lot of nothing. Early in my career, I worked for this old German guy
01:36:48.480 and whenever I would ramble like that, he'd be like, "Say what they're going to say and then shut the [ __ ] up." And I
01:36:54.639 ID advice to me. I've told you that before, right?
01:36:59.679 So, we can kind of fast forward to today. Ultimately, with Sam back at the helm, Open AI just continued to double,
01:37:05.119 triple down, buoied by President Trump's big, beautiful bill that basically takes
01:37:10.960 off all the reigns for AI. I mean, investing billions and billions of dollars of US
01:37:17.760 taxpayer money into these AI companies and subsidizing them. Chat GPT5 just
01:37:22.960 came out I think last week. Open AI said that they were not going to release information on the energy use of TAT
01:37:30.239 GPT5. Oh, and obviously there's no laws that compel them to do so.
01:37:36.639 Apparently there's been a lot of backlash against Chachi BT5. It's not as
01:37:42.400 lawy as the old one would be. So apparently, I mean, again, as you guys know, I don't use chat GPT at all. Um,
01:37:48.639 but apparently if you were to ask it questions before, I'd be like, "That's a really good question. You must be really smart for asking that question." And
01:37:55.360 there now it's like, "You [ __ ] idiot. You should know this already." Nice one, Boner. Yeah, I want to pull it
01:38:00.880 up. Turn it on bullying mode. Um, there with chat GPT4, there were people
01:38:06.320 actually, if you go to a subreddit called AI is my boyfriend, there are people who have been engaged to their
01:38:12.320 AI. it's their best friend. Um, and I think that they turned down that
01:38:18.159 capability, so it's not as friendly anymore. It's more like a Google search. Um, I think that there was enough
01:38:23.920 articles written about people going suicidal because they thought that this AI was going to break up with them. Or
01:38:29.199 remember the kid I think he was a kid, right, whose AI told him that he was they were Games of Thrones character and
01:38:35.119 they said like, "If you kill yourself, you'll meet me in the afterlife." Oh jeez. Yeah. And so there's a lot of these
01:38:40.880 instances where people get kind of what they call AI psychosis. Um, and they believe that this AI is a real person.
01:38:46.960 But remember guys, the AI doesn't know what it's saying. It's just predicting text that it thinks you want to hear
01:38:53.360 based on information from the internet. There's also been some funny drama this week. Did you see this? The whole thing
01:39:00.000 about the charts that they posted these charts being like this is how much bigger it is compared to uh the previous
01:39:06.080 models. But the um but the bars are all [ __ ] up. So like they use ChachiBT to
01:39:11.760 create these charts, but like here you're seeing a bar that's about that says 52.8 and it's taller than the bar
01:39:19.119 that says 69.1, which is the same size as the bar that says 30.8. So it's like
01:39:25.760 I would be upset if an intern gave me this chart. I think a 10year-old could do better job. But imagine like they
01:39:32.960 botched that in the introduction of this product. Sam Alman said that GGPT5 would have PhD
01:39:40.159 level intelligence, but it's unable to solve basic math problems. Um, Alman also has a new company that's
01:39:48.000 uses eye scanning orbs to detect if you're human or not. Um, we're actually going to cover that in the after show.
01:39:53.760 Are you ready to move to the takeaways? Uh, yes. I think it is interesting to me because
01:40:01.520 we're kind of at a moment in time where we have all these men in power, Sam Alman, Elon Musk, Donald Trump, who just
01:40:07.280 want to be loved. They just want to be liked. And I think for a lot of them
01:40:13.360 right now, their decision-m and motivation comes back to how can I get the most people to like me right now?
01:40:20.000 And I think it's interesting because it's like that's also why people who use AI in most maladaptive ways. Like it
01:40:27.600 comes up with Mark Zuckerberg. People only have three friends. Like if men could just get friends, a lot of these
01:40:33.920 problems would be solved. That's your takeaway? Well, I just think Sam Alman ultimately
01:40:39.760 like he's lying and he's trying to grow and build this empire because he wants to be as good as he doesn't want to
01:40:46.159 disappoint Peter Graham. like he doesn't want to disappoint his Sam uh Aaron mentioned that Paul Graham is basically
01:40:52.560 a father figure to Sam at this point and I think he probably has a big motivation of like I don't want to disappoint dad
01:40:59.119 so I know it always comes back to this but I really do think he doesn't want to disappoint his male mentors and I think
01:41:06.960 that's what it comes down to. I think we're all this lying and empire building because ultimately that's what this book was about. I don't know that Sam Alman
01:41:13.760 cares that much about AI in particular. He cares about building an empire. He cares about being in power. He cares
01:41:20.239 about collecting followers. He idolizes Napoleon. I mean, it it's
01:41:26.000 just so obvious. It's so simple. That's my takeaway. Okay. What's your takeaway?
01:41:31.520 I'm thinking about what needs to happen to make me less angry, less scared of
01:41:38.880 AI. And for those things to happen, essentially Open AI needs to
01:41:46.080 we need to cut the head off the horse. And that is like a complete clear house clearing. So Altman's got to go.
01:41:54.560 Sam Alman has given us no reason to believe anything he says
01:42:00.880 or believe that he's going to do anything that is for the common good.
01:42:08.320 since chat GPT has been released.
01:42:13.440 If you I think you're with the way you put it where it's like if you become so dependent on chat GPT your brain will be
01:42:20.239 atropied. That's the way it's going to work. And eventually guys, they're going to start wanting to make money on this.
01:42:26.000 And also they're going to start putting advertisements on there. I'm I'm I'm on
01:42:31.679 calls right now to talk talking about gen GEO, which I think is like
01:42:37.760 generative uh engine optimization. Well, like search engine optimization.
01:42:42.800 Exactly. Yeah. You don't think that you're going to start to look up a history fact and it's going to subtly, you know, about an
01:42:50.239 aviation history fact and it's going to suddenly given you an advertisement for Delta. I mean you guys like it's it's
01:42:56.800 going to the the only motivation here is profit. And I know people think well I will have to work less and maybe with AI
01:43:03.199 we could even move to a shorter work week. Well let me tell you guys that's a policy decision that has nothing to do
01:43:09.679 with productivity. Those gains will be captured only by people at the top.
01:43:16.560 So yes Altman needs to leave and then ultimately we need government intervention. I'm looking at you,
01:43:24.080 Europe, and the EU. You are going to be the thought leaders on this. Sweden, you two,
01:43:31.119 like we need you to lead the way on this because we clearly are not in a place to
01:43:36.639 to do it and it's really unfortunate, but yeah, it's it's scary and like think
01:43:42.080 about all the things that uh the the the pros do not outweigh the cons. your
01:43:47.199 energy bill. The New York Times just put out a article today that the reason your energy bills are so high is because of
01:43:52.719 these data centers, all the human rights things that are happening and we will cover all that before you
01:43:58.400 can go into it briefly, but so much more. We're going to do full episodes on this,
01:44:04.400 you know, and I'd love if listen, here's the thing. If you are somebody that's optimistic about AI and you have like a,
01:44:11.360 you know, solid background on this, you know about energy consumption, you know about like I don't know if you're coming
01:44:19.119 from a place of academic knowledge, I'd love to hear from you. Like give us some
01:44:24.159 good reasons why we shouldn't be so pessimistic. Um, one thing that keeps me optimistic is knowing that these guys
01:44:30.080 are so dumb. And I do think that like for example, people are already realizing that some of these smaller
01:44:36.159 models that can run on your phone for example that are more highly trained might actually be more effective than a
01:44:42.639 super large data model than a super large language model that requires a data center. So it might be possible
01:44:49.600 that in short order we'll realize that that maybe was overly we scaled too
01:44:54.719 much. Yeah. And then once we hit a dust bowl, believe me, what does your sister or
01:45:00.320 your sister, my sister-in-law, your wife always says? People only change. What does she say about people changing to
01:45:05.840 the degree of pain that they're in? Yeah. We're not in enough pain, unfortunately. And I don't think anything will change
01:45:11.520 until we are. So once the dust bowl comes, people didn't stop plowing until things got really bad. And I know it
01:45:18.000 sucks, but the one thing that makes me feel better is humans are going to human. And there's nothing that I can do
01:45:23.199 in particular other than learn and grow and share. Um but sometimes I feel really guilty and I
01:45:29.600 feel really scared and then I remember whether I was here or not all this stuff would have happened. Right.
01:45:34.639 Yeah. Yeah. And so all I can do is use my voice and all I can do is like anytime that there's a protest anytime listen Arizona
01:45:41.360 just uh prevented a data center from being built and that was incredible and I'm really proud of them. Did you hear
01:45:47.600 that? Mhm. So there are things there are people fighting against this and you should join them. It's really fun to get
01:45:52.800 out and protest especially when it's when it's nice weather out. It's a great way to get your steps in. Not in Arizona in August.
01:45:59.119 But isn't it amazing that they did it in August? That that even just goes to tell you. So hey, by the way, our good news
01:46:05.920 correspondent is actually back from her internship and she has submitted a story. So before I read the story, let's
01:46:11.760 just say goodbye now. Um and then we'll end with the story. Thank you guys so much. We will be back in two weeks. So,
01:46:17.679 if you want to hear from us before then, you can join the Patreon over on the Patreon for $5 a month. You get four
01:46:25.040 extra episodes every month. You get our after show, which is where we kind of cover extra stories that we didn't have
01:46:30.320 time to cover on the main episode, and then you get our hot topics. Last week, we covered a big story, bombshell
01:46:36.800 investigation by the New York Times about Uber covering up sexual assault reports. Uber, why combinator company? Yeah,
01:46:44.159 I don't know. Look up. I'm not sure. Um, but yeah, so lots of good stuff over
01:46:50.639 there. Remember this is an indie podcast and we are supported by listeners like you and then of course our advertisers
01:46:56.080 who are also listeners like you. No, they were not in my company. Okay, great. If you want to follow us on
01:47:01.119 Instagram or Tik Tok, you can do so at corporate gossipodcast. And if you want to feel a little bit better, listen to
01:47:09.040 this story from our good news correspondent Elizabeth Lou. Last May, Buonaf Facetta, a restaurant in San
01:47:15.040 Diego, was raided by ICE agents wearing tactical gear. They handcuffed several workers and detained four, and as a
01:47:21.840 result, the restaurant was forced to temporarily close several locations. Now, the city is fighting back. On July
01:47:29.199 22nd, the county board of supervisors voted to launch a Know Your Rights program for small businesses. The
01:47:35.840 program will be developed in collaboration with worker advocates, other small businesses, and legal
01:47:40.880 experts. It will provide information on how to respond if ICE agents show up, how to avoid business closure, and how
01:47:47.760 to protect workers due process rights. The program will be offered in multiple languages and will be available
01:47:53.760 virtually. Board chair Tara Lawson Remmer crafted the proposal after the raid of the restaurant. She said, "Every
01:48:00.880 business owner and every worker has rights. Due process does not disappear based on where you were born." The first
01:48:07.199 workshop was held in person on August 14th, and another will be held over Zoom on Monday, August 18th from 6:00 to 7:30
01:48:15.119 p.m. If you would like to attend, you can actually RSVP in the link in our show notes. Even if you don't live in
01:48:21.119 San Diego, there are still many ways you can help. You can call your state representatives and tell them that you
01:48:26.719 don't support Trump's actions, and you can advocate for the immigrants in your community. You can support immigrant
01:48:32.639 businesses around you. You can look into organizations that already exist in your neighborhood. You can volunteer at a
01:48:39.199 legal aid center, help teach an ESL class, or volunteer for an afterchool program. Lastly, if you have immigrants
01:48:46.480 living in your neighborhood, don't be afraid to reach out and offer a helping hand where needed. Thanks, Elizabeth,
01:48:52.639 for submitting this
